---
tags:
  - 数学
  - 信息学中的概率统计
  - AI
toc: "true"
categories: 笔记
---

5分钟必不可能读完

>说明
>本文写于2025年春季学期的《信息学中的概率统计》课程上，但是具体是什么时候，忘了。故暂定时间为5月31日。

本笔记只记录易混淆的重点内容

# Chap 1
## 有限可加和可列可加  

![](/assets/images/Pasted image 20250221102103.png)

## 条件概率的全概率公式

![](/assets/images/Pasted image 20250221112212.png)

## 贝叶斯公式易错题

![](/assets/images/Pasted image 20250221114453.png)

第二问易错。直觉上的答案是 1/2，但是正确答案是 2/3

## 事件独立是有等价条件的

![](/assets/images/Pasted image 20250221115427.png)

## 偷感很重啊嗯

![](/assets/images/Pasted image 20250228104101.png)

# Chap 2

泊松分布、正态分布、几何分布

![](/assets/images/Pasted image 20250328101522.png)

注意有一个长成这样的公式!!

$$
e^{\lambda}=\sum_{j=0}^{\infty}\frac{\lambda^j}{j!}
$$

![](/assets/images/Pasted image 20250321102409.png)

![](/assets/images/Pasted image 20250328101735.png)

![](/assets/images/Pasted image 20250413105758.png)

# Chap 3

key words:

离散型随机向量

联合概率分布 $P(X=x_i,Y=y_j)$

条件概率分布 

$$
P(X=x_i|y=y_j)=\frac{P(X=x_i,y=y_j)}{P(Y=y_j)}
$$

边缘分布 $P(X=x_i)$

## 分布函数

![](/assets/images/Pasted image 20250314115750.png)

边缘分布函数

![](/assets/images/Pasted image 20250314115854.png)

条件分布函数

![](/assets/images/Pasted image 20250314115921.png)

## 概率密度相关

![](/assets/images/Pasted image 20250320201359.png)

![](/assets/images/Pasted image 20250320201341.png)

![](/assets/images/Pasted image 20250320201430.png)

二元正态分布

![](/assets/images/Pasted image 20250320200543.png)

![](/assets/images/Pasted image 20250402202649.png)

![](/assets/images/Pasted image 20250414151426.png)

## 独立性

![](/assets/images/Pasted image 20250320200612.png)

![](/assets/images/Pasted image 20250320203835.png)

## 多元随机变量的函数 $\mathbf{Y}=g(\mathbf{X})$

和的分布

![](/assets/images/Pasted image 20250321101427.png)

商的分布

![](/assets/images/Pasted image 20250401135328.png)

$$
\mathcal{ABCDEFGHIJKLMNOPQRSTUVWXYZ}
$$

![](/assets/images/Pasted image 20250321102120.png)

上面式子有一个地方写错了，乘积那里应该是 $f_X*f_Y$

一个例题。

![](/assets/images/Pasted image 20250321104128.png)

看不懂怎么变化的就去看 [Gauss积分](https://tianfulvye.github.io/Gauss%E7%A7%AF%E5%88%86/) 的例题喵

![](/assets/images/Pasted image 20250321104747.png)

卡方分布

![](/assets/images/Pasted image 20250321105704.png)

min-max 分布（从特殊到一般）

特殊

![](/assets/images/Pasted image 20250321112339.png)

一般

![](/assets/images/Pasted image 20250321112618.png) 

## 密度变换公式 $f_\mathbf{Y}(\mathbf{y})=f_\mathbf{X}(\mathbf{h}(\mathbf{y}))|J(\mathbf{y})|$

![](/assets/images/Pasted image 20250321114921.png)

一元情形下，$\frac{dx}{dy}=|h'(y)|$ ，其中 $h$ 是 $g$ 的反函数，这就是第二章连续随机变量的函数分布定理.

例题见我刚刚写完就看到密度变换公式的笔记：[雅可比行列式与换元法](https://tianfulvye.github.io/%E9%9B%85%E5%8F%AF%E6%AF%94%E8%A1%8C%E5%88%97%E5%BC%8F%E4%B8%8E%E6%8D%A2%E5%85%83%E6%B3%95/)

橙色的块块里面那个 $h(y)=g^{-1}(y)$ 应该是 $x=h(y)=g^{-1}(y)$（不加粗了，懒）

# Chap 4

### 数学期望 （理解记忆）

离散的：

![](/assets/images/Pasted image 20250328110713.png)

需要注意这个东西乘了一个 $x_i$

同时它还等于下面这个

![](/assets/images/Pasted image 20250328110811.png)

上面这个东西理解性记忆

连续的：

![](/assets/images/Pasted image 20250328110832.png)

**注意 $E(x)$ 的积分里面有一个 x，** 如果没有的话就成了分布函数 $F(x)$ 了

下面是几个例子（泊松分布、几何分布）

![](/assets/images/Pasted image 20250328101817.png)

其中下面这张图的 $X\sim P(\lambda)$ 就是上面的 $X\sim \pi (\lambda)$

也就是泊松分布

![](/assets/images/Pasted image 20250328105721.png)

算这些复杂的数学期望经常用到[换函数法]( https://tianfulvye.github.io/%E6%8D%A2%E5%87%BD%E6%95%B0%E6%B3%95/).

数学期望和分布函数

![](/assets/images/Pasted image 20250328111011.png)

![](/assets/images/Pasted image 20250328111555.png)

上面这个意思就是 $Y=g(x)$ 的这个鬼变换可以直接乘到 $E(Y)$ 里面去，比较方便。

>  定理的意义在于不用先求Y的分布再算均值

对于不管是离散的，还是连续的，随机变量，还是随机向量，都有这个变换

这个好的性质让我们可以迅速算出这个例题。

![](/assets/images/Pasted image 20250328112036.png)

当然这个题是用的是这个定理的离散版：（其实就是从概率乘 x 换成乘 g(x)）

$$
E(Y)=E(g(X))=\sum_{j=-\infty}^{+\infty}P(X=j)\cdot g(X)dx
$$

![](/assets/images/Pasted image 20250328113354.png)

注意看这个 3 和 5，其中 3 不需要变量相互独立，5 是乘法可分性。

期望还有一个非常重要的性质，但是那个垃圾课件就像不知道一样直接略过了它，在下面补充一下。

$E[XY]$其实是可以通过联合概率密度函数直接积分求得的

![](/assets/images/Pasted image 20250413230433.png)

狗屎一坨的例题

![](/assets/images/Pasted image 20250328113848.png)

### 示性函数

![](/assets/images/Pasted image 20250328115420.png)

上面谈到了有一个式子

$$
E(x)=\sum_{n=0}^{+\infty}P(X> n)
$$

这是离散的时候的期望计算公式，现在

$$
E(1_A)=P(A)
$$

 所以

![](/assets/images/Pasted image 20250413112013.png)

更好的视角是把示性函数看做一种方法或者理念，而不是一个定义，比如下面这道题

![](/assets/images/Pasted image 20250402185429.png)

还有这道题

![](/assets/images/Pasted image 20250402185923.png)

以上两道题都基于示性函数的另一个性质

如下

![](/assets/images/Pasted image 20250413112129.png)

### 方差

![](/assets/images/Pasted image 20250402191455.png)

懂的都懂啊嗯

方差还有一个变形，就是 $D(X)=E(X^2)-E(X)^2$，推导如下啊嗯

![](/assets/images/Pasted image 20250402191651.png)

常用方差

其中 $D(X)=npq$ 里面的 $q=1-p$

![](/assets/images/Pasted image 20250402192734.png)

![](/assets/images/Pasted image 20250402192752.png)

补充一个指数分布的，

指数分布 $Exp(x)$ 期望：$\lambda$，方差：$\lambda^2$

方差还有一些性质

![](/assets/images/Pasted image 20250402193831.png)

### 协方差 $Cov(X,Y)$

$$
Cov(X,Y)=E[(X-E(X))(Y-E(Y))]
$$

还有一个叫 $相关系数$ 的东西

$$
\rho_{(XY)} =\frac{Cov(X,Y)}{\sqrt{D(X)D(Y)}}
$$

常用结论：

$E(X\cdot Y)=E(X)E(Y) + Cov(X,Y)$ 并且当 X,Y 独立的时候协方差等于 0,此时 $E(X,Y)=E(X)E(Y)$

注意期望的性质如果是 $E(X+Y)$ 就直接等于 $E(X)+E(Y)$

上面这个式子还有一个变式

$Cov(X,Y)=E(X\cdot Y)-E(X)E(Y)$

$D(X+Y)=D(X)+D(Y)+2Cov(X,Y)$

还有下面这个飞舞公式

![](/assets/images/Pasted image 20250402194744.png)

![](/assets/images/Pasted image 20250402195408.png)

**相关系数**-related:
![](/assets/images/Pasted image 20250402201444.png)
![](/assets/images/Pasted image 20250402201459.png)

**独立一定不相关，但是不相关不一定独立**
例子马上到来：
![](/assets/images/Pasted image 20250402201727.png)


### 高阶矩&特征函数

![](/assets/images/Pasted image 20250411110954.png)

![](/assets/images/Pasted image 20250411111018.png)

![](/assets/images/Pasted image 20250411111037.png)

![](/assets/images/Pasted image 20250411111119.png)

![](/assets/images/Pasted image 20250411111143.png)

![](/assets/images/Pasted image 20250411111205.png)

![](/assets/images/Pasted image 20250411111227.png)

## 多元随机变量的数字特征（数字特征就是第四章上面那一坨）

![](/assets/images/Pasted image 20250411111545.png)

理解记忆上面这个构式协方差矩阵：

![](/assets/images/Pasted image 20250411111828.png)

**接下来我们看看这个有用的协方差矩阵是怎么发挥实际作用的**
考虑二元正态分布 $(X_1, X_2)$，其中概率密度为  

$$
f(x_1, x_2) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left[ \frac{(x_1 - \mu_1)^2}{\sigma_1^2} - 2\rho \frac{(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1\sigma_2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2} \right]},
$$

协方差矩阵为  

$$
B = \begin{pmatrix}
\sigma_1^2 & \rho\sigma_1\sigma_2 \\
\rho\sigma_1\sigma_2 & \sigma_2^2
\end{pmatrix}
$$

行列式为  

$$
|B| = \sigma_1^2\sigma_2^2 (1 - \rho^2)
$$

$B$ 的逆矩阵为  

$$
B^{-1} = \frac{1}{1-\rho^2} \begin{pmatrix}
\frac{1}{\sigma_1^2} & -\frac{\rho}{\sigma_1\sigma_2} \\
-\frac{\rho}{\sigma_1\sigma_2} & \frac{1}{\sigma_2^2}
\end{pmatrix}
$$

同时  
![](/assets/images/Pasted image 20250411113032.png)

故**二元正态分布的概率密度可写成**  
![](/assets/images/Pasted image 20250411113055.png)

这还不够，还有多元的

![](/assets/images/Pasted image 20250411112611.png)

称满足 $a = 0$, $B = I_d$ 的正态分布为 **$n$ 维标准正态分布**，此时

$$
f(x) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x_i^2}{2}\right)
$$

恰为 $n$ 个相互独立的一维标准正态的联合密度。

还有一些沟槽的定理

![](/assets/images/Pasted image 20250411115718.png)

上面之中的第四条是尤为重要的

![](/assets/images/Pasted image 20250411115740.png)

证明略

还有个例题。。我操这啥啊

![](/assets/images/Pasted image 20250411115641.png)

# Chap 5

## 切比雪夫不等式、大数定律

![](/assets/images/Snipaste_2025-04-25_11-25-11.png)

如图所示，

首先有个概念叫依概率收敛，它是理解 2 的基础。

![](/assets/images/Pasted image 20250425113526.png)

**依概率收敛**你可以把它想象成日常生活中投飞镖的场景。假设靶心是一个确定的目标值，而你每次投飞镖的位置代表一个随机变量的取值。一开始，你可能投得离靶心很远，但经过不断练习后，你投出的飞镖会越来越集中在靶心周围。**虽然偶尔还是会有飞镖偏离靶心，但这种偏离的次数和幅度会越来越少**，偏离靶心一定距离的概率会越来越小，最终大部分飞镖都会聚集在靶心附近。这个过程就像随机变量依概率收敛到某个值，就像你通过练习，使飞镖越来越可能落在靶心周围一样。

直观理解这 5 个公式，如下表所述。

**注：独立性更强！相互独立一定不相关，不相关不一定相互独立。**
![](/assets/images/Pasted image 20250515114934.png)

| 公式          | 理解                                                                                                                                              |
| ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| 切比雪夫不等式     | 随机变量 $X$ 偏离期望 $E(X)$ 的程度，遵循一定的概率，这个概率跟 $D(X)$ 和偏离距离（偏离程度）$\epsilon$ 有关。                                                                         |
| 伯努利大数定律     | 做无数次实验（eg.掷硬币），在这无数次实验过后，频率等于概率（亦即频率 $\frac{m_n}{n}$ 依概率收敛到概率 $p$）。                                                                             |
| 切比雪夫大数定律    | 条件：$X_1,\cdots,X_n$ **不相关**，而且都有期望和方差。不要求同分布。当有无限个这种分布的时候（$n\to+\infty$）， $X_i$ 求和后的平均值 $\frac{1}{n}\sum_{i=1}^{n}X_i$ 依概率收敛到 $X_i$ 期望求和后的平均值。  |
| 切比雪夫大数定律的推论 | 略                                                                                                                                               |
| 辛钦大数定律      | 条件：$X_1,\cdots,X_n$ **独立**，并且必须同分布。要求有期望 $\mu$，不要求有方差。当有无限个这种分布的时候（$n\to+\infty$）， $X_i$ 求和后的平均值 $\frac{1}{n}\sum_{i=1}^{n}X_i$ 依概率收敛到期望 $\mu$。 |

## 中心极限定理

![](/assets/images/Snipaste_2025-05-09_10-45-19.png)

> 想象你有一大袋各种颜色的糖果，每次随机抓一小把（比如 10 颗），然后数一数其中红色糖果有多少。重复抓很多次后，你会发现每次抓的红色糖果数量虽然不同，但整体会围绕某个“中间值”波动（比如平均每次抓 3 颗红糖果）。更神奇的是，如果你把这些结果画成柱状图，形状会越来越像一个对称的“钟形”（就像倒扣的帽子）。这就是中心极限定理的核心——**即使原始数据乱七八糟，只要多次随机取样，这些样本的平均值总会乖乖排成钟形图案**。比如全班同学的身高、你每天的通勤时间，甚至披萨外卖的迟到分钟数，都符合这个规律。

棣莫弗-拉普拉斯定理还有一种表示形式

如下，它把 $X_n$ 拆成 $X_i$ 的加和了

![](/assets/images/Pasted image 20250515152555.png)

补充一个定理：**林德伯格列维定理**

![](/assets/images/Pasted image 20250515152450.png)

优质例题一道啊嗯

![](/assets/images/Pasted image 20250515160948.png)

## 马尔科夫不等式、马尔科夫大数定律

![](/assets/images/Pasted image 20250509105128.png)

>想象你是一个班主任，班上有 100 个学生，他们的零花钱平均每周是 50 元。这时候有个“土豪”同学跳出来说：“我每周零花钱至少有 500 元！”  
>
>马尔可夫不等式就像个“侦探”，它会告诉你：**土豪同学最多只可能有 10%**（因为 500 元是平均 50 元的 10 倍，所以比例最多是 1/10）。  
>
>简单来说，**它限制了“极端值”出现的概率**——某个数值越大，出现的概率就越小。就像“班里不太可能有太多学生零花钱是平均的 10 倍以上”，或者“你不太可能天天中彩票”。它不关心具体分布，只给出一个宽松但保险的上限**～  
>
> （喵附注：这个侦探有点粗线条，实际概率可能更低，但它保证“至少不会超过这个数”！）

![](/assets/images/Pasted image 20250509105519.png)

可以注意一下这个证明，它（再次）说明了一个问题：

$$
\int|x|^kf(x)dx=E(|x|^k)
$$

这个等式是 trivial 的，但是我不知道为啥老是想不到...

![](/assets/images/Pasted image 20250509113142.png)

注：这个 $lim$ 的格式跟切比雪夫大数定律是完全相同的。不同的是约束条件。切比雪夫的要求是有期望，有方差，不相关（独立），无需同分布。

马尔科夫的是这个诡异的东西，对它的理解如下：

**用小猫吃鱼解释马尔可夫大数定律的条件**：  

>想象你有 $n$ 只小猫，每只每天随机吃到 $X_i$ 条鱼（比如有的吃 2 条，有的吃 0 条，平均每只吃 1 条）。$D(\sum X_i)$ 表示所有小猫吃鱼总量的波动程度。  
>
>图片里的条件 

$$
\lim_{n\to\infty} \frac{1}{n^2} D(\sum_{i=1}^n X_i) = 0
>
$$

> 其实在说：  
>**当小猫数量$n$超级多时，全体吃鱼总量的波动（方差）增长速度必须远慢于$n^2$**。  
>
>为什么需要这个条件？  
>1. **防止“巨猫霸权”**：如果某只小猫每天吃$n^2$条鱼（方差爆炸增长），它会主导整个鱼群总量，导致平均值不稳定。  
>2. **保证“猫多势众”**：只有波动被$n^2$压制，大量小猫的随机性才能互相抵消，最终平均值才会稳稳收敛到期望值（比如所有猫平均每天吃 1 条鱼）。  
>
>（喵式总结：这个条件像“鱼塘管理法则”——可以有个别大胃王，但整体不能失控，否则猫均吃鱼量永远算不准！）  
>
> *附：对比辛钦大数定律（要求同分布），马尔可夫大数定律的这个条件更宽松，允许小猫们吃鱼量分布不同，只要波动不集体暴走就行~*

例题见 PPT

## 收敛性

几乎必然收敛 $\to$ 依概率收敛 $\to$ 依分布收敛

反之不成立

### 几乎必然收敛  

**公式**：

$$
P(\lim_{n\to\infty}X_n=X)=1
$$

**生活化比喻**：  
想象你每天用同一个骰子投掷无数次。如果“几乎必然收敛”发生，就相当于你赌咒说：“只要我掷的次数足够多，最终**绝对**会连续出现 100 次 6 点”——虽然听起来疯狂，但这个承诺在无限次尝试中确实会兑现。就像你确信只要时间足够长，猴子打字机**必定**能打出《莎士比亚全集》。  

**专业解释**：  
数学上，随机变量序列{Xₙ}“几乎必然收敛”到 X，指的是在所有可能实验结果中（除开一个概率为零的例外集合），当 n 越来越大时，Xₙ(ω)和 X(ω)的差距最终**永远**缩小到零。比如，强大数定律说“样本均值几乎必然收敛于期望”，意味着实验重复无限次后，样本均值和真值的偏差**必定**消失（除非遇到概率为零的极端情况）。  

---  

### 依概率收敛  
**专业解释**：  
{Xₙ}“依概率收敛”到 X，是指对任意小的容差ε>0，当 n 趋近无穷时，Xₙ落在 X 的ε邻域外的概率**趋近于零**。比如弱大数定律中，样本均值依概率收敛于期望，意味着虽然偶尔会有样本均值偏离真值，但偏离的概率随样本量增大而**无限降低**。公式表示为：  

$$
 \lim_{n\to\infty} P(|X_n - X| \geq \epsilon) = 0
$$

![](/assets/images/Pasted image 20250515112821.png)

![](/assets/images/Pasted image 20250515112903.png)

注 1：并集就是满足一个就全集为 1

注 2：

依概率收敛**不能**写成 $\lim_{n\to\infty}P(X_n=X)=1$

![](/assets/images/Pasted image 20250523143041.png)

注 3.

$\lim$ 不能提到括号里面，这是老祖宗几千年来传下的规矩，不能变！

---  

### 依分布收敛  

![](/assets/images/Pasted image 20250515113247.png)

![](/assets/images/Pasted image 20250515113225.png)

## 集中不等式

一个不等式叫霍夫丁不等式，见下：

![](/assets/images/Pasted image 20250515185114.png)

**第一段（生活化比喻）：**  
想象你有一个装满了红球和蓝球的袋子，但不知道具体比例。你每次伸手随机抓10个球，记录红球的比例。重复多次后，发现结果有时高有时低。Hoeffding不等式就像告诉你：无论袋子里实际比例是多少，你抓的样本比例和真实比例相差很大的概率非常小。就像天气预报说“明天有90%概率不会下暴雨”，它给你一个可靠的边界，让你对抽样结果更有信心。  

**第二段（专业解释）：**  
Hoeffding不等式是概率论中一个工具，用来量化“抽样结果偏离真实值多远”的可能性。比如真实比例是$p$，抽样$n$次得到比例$\hat{p}$，Hoeffding不等式说：$P(|\hat{p} - p| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$。这里$\epsilon$是你允许的误差（比如0.05），$n$是抽样次数。公式右边像“概率保险丝”——随着$n$增大或$\epsilon$变严，超出误差的概率指数级下降。它不依赖真实分布，只要求抽样是独立的（像每次抓球后放回），所以广泛用于统计和机器学习。

注意到在专业解释这一段里面给出的 Hoeffding 不等式和图里面的是不一样的，为什么呢？原因见下：（一般形式就是指蓝色图片里面那个难一点的公式。）

![](/assets/images/Pasted image 20250517214134.png)

证明见这篇文章： [不等式的证明](https://tianfulvye.github.io/Hoeffding%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E8%AF%81%E6%98%8E/)

但是 Hoeffding 不等式是有界的，这让人有点头大，去掉有界的条件，$X_i$ 需要满足什么样的条件才能满足类似于 Hoeffding的不等式？

![](/assets/images/Pasted image 20250515191442.png)

![](/assets/images/Pasted image 20250515191537.png)

这个新的不等式就是把 Hoeffding 不等式里面的 $(b_i-a_i)^2$ 换成了 $\sigma_i^2$，**并且**还把分子的 2 移到了分母。

再来看 Subgaussian 分布的性质，

![](/assets/images/Pasted image 20250515191939.png)

![](/assets/images/Pasted image 20250515192000.png)

## 做题心得

估计概率 $\to$ 中心极限定理

两个问题

**相等到底怎么判定**

我觉得判定两个数 A 和 B 相等应该就是对于 $\forall \epsilon \geq 0$  都有 $|A-B|\leq\epsilon$ ，

如果上面说的是对的，那依概率收敛为什么不能写成 $\lim_{n\to\infty}P(X_n=X)=1$ 呢？同时，为什么几乎必然收敛又能写成 $P(\lim_{n\to\infty}X_n=X)=1$，同时还能写成 $\lim_{n\to\infty}P(\cup_{m=n}^{+\infty}A_m(\epsilon))=0$？

**第二个 几乎必然收敛**
要求是

$$
\lim_{n\to\infty}P(\cup_{m=n}^{+\infty}A_m(\epsilon))=0
$$

其中 $A_n$ 是这个东西

$$
A_n(\epsilon)=\{|X_n-X|\geq \epsilon \}
$$

那么是否可以有如下论断：

$$
因为 P(\cup^{\infty}_{k=1}A_k(\epsilon))\leq \cup^{\infty}_{k=1}P(A_k(\epsilon))\leq \sum^{\infty}_{k=1} P(A_k(\epsilon))
$$

且根据 B-C 引理可知，$\sum^{\infty}_{k=1} P(A_k(\epsilon))$ 有限时，等价于 $X_{n}几乎必然收敛于 X$，所以证明 $\sum^{\infty}_{k=1} P(A_k(\epsilon))$ 有限，就证明了 $\lim_{n\to\infty}P(\cup_{m=n}^{+\infty}A_m(\epsilon))=0$。

这对吗？

答案：对

几乎必然收敛的核心在于，尽管在无限序列中可能存在有限次的偏离，但无限次偏离的概率为零。

# Chap 6
## 统计量

![](/assets/images/Pasted image 20250611211910.png)

统计量的无偏性：$E(\hat{X})=E(X)$

## 卡方分布（正态加一起）

![](/assets/images/Pasted image 20250611212049.png)

这里不得不提到[卡方分布、指数分布还有伽马分布的关系](https://tianfulvye.github.io/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83-%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E8%BF%98%E6%9C%89%E4%BC%BD%E9%A9%AC%E5%88%86%E5%B8%83%E7%9A%84%E5%85%B3%E7%B3%BB/)了，总结起来就是以下几点：

$$
\text{Exp}(\lambda)=\Gamma(1,\lambda)
$$

$$
\chi^2(n)=\Gamma(\frac{n}{2},\frac{1}{2})
$$

$$
\frac{\Gamma(a,b)}{c}=\Gamma(a,b \cdot c)
$$

## t 分布（正态除以根号下 n 分之卡方）

![](/assets/images/Pasted image 20250611213105.png)

## F 分布（两个【卡方除以 n】相除）

![](/assets/images/Pasted image 20250611213032.png)

为一坨很长的东西 不用管

# Chap 7
## 矩估计

就是说 分布里面有几个参数（通常是 1 个、2 个）不知道，然后**用样本的k阶矩或k中心阶矩来估计参数**。

![](/assets/images/Pasted image 20250611213428.png)

看一个题就一目了然了

![](/assets/images/Pasted image 20250611213543.png)

这个题目让估算 $\theta$，然后写出 $E(x)$ 的形式，用 $E(x)$ 去表示 $\theta$，就这么简单

同时，这里的 $\mu_1$ 不是很常见，一般直接写 $\bar{X}$，如图所示，这是另一道题

![](/assets/images/Pasted image 20250611213728.png)

## 极大似然估计

![](/assets/images/Pasted image 20250611214058.png)

老师你写的课件不行啊，还是得看我们 Hao Song 的板书我说

例题来一道

![](/assets/images/Pasted image 20250611214159.png)

## 估计量的标准

![](/assets/images/Pasted image 20250611214251.png)

![](/assets/images/Pasted image 20250611214304.png)

![](/assets/images/Pasted image 20250611214323.png)

注意，有偏估计也可能满足相合性；“相合是否渐进无偏？渐进无偏是否相合？”的答案都是不一定。

![](/assets/images/Pasted image 20250612172635.png)

## 区间估计

枢轴量法，名词很抽象，反正就是我们估计了一个参数，我们想看这个参数的估计的准确度是多少（e.g.是不是大于 90%），我们就... 哎呀说不清道不明，直接看题吧

![](/assets/images/Pasted image 20250611215226.png)

就这个意思

![](/assets/images/Pasted image 20250611215253.png)

![](/assets/images/Pasted image 20250611215304.png)

![](/assets/images/Pasted image 20250611215340.png)

# Chap 8

假设检验

## 假设检验的基本概念

假设有一种植物，到了春天能开一堆花，我们统计玩不同株的开花数量之后，做出假设：“这种植物开花数量的均值是 $\mu_0$”，假设检验的目的就是看看这句话说对没有（比如 95%的可能性，就认为说对了）。

![](/assets/images/Pasted image 20250611215754.png)

上面这个例子里面，我们一般认为 $\mu=\mu_0$ 是原假设；$\mu\neq\mu_0$ 是备择假设。

![](/assets/images/Pasted image 20250611220001.png)

![](/assets/images/Pasted image 20250611220056.png)

![](/assets/images/Pasted image 20250611220117.png)

![](/assets/images/Pasted image 20250611220143.png)

![](/assets/images/Pasted image 20250611220208.png)

## 检验一个正态总体
### U 检验（也叫 Z 检验）： $\sigma^2$ 已知 检验 $\mu$

![](/assets/images/Pasted image 20250611220514.png)

![](/assets/images/Pasted image 20250611220543.png)

这上面就是 U 检验的具体步骤说明

课件上是这样说的 其实是一个意思

![](/assets/images/Pasted image 20250611220705.png)

注意 $\sigma$ 要除以根号 n，这是因为 $\bar{X}$ 的方差就是 $\sigma$ 除以根号 n

众所周知不见到题目浑身燥热，所以直接上题

![](/assets/images/Pasted image 20250611221228.png)

### T 检验： $\sigma^2$ 未知 检验 $\mu$

![](/assets/images/Pasted image 20250611220903.png)

### 卡方检验：$\mu$ 已知或未知 检验 $\sigma^2$ 

![](/assets/images/Pasted image 20250611221448.png)

![](/assets/images/Pasted image 20250611221608.png)

## 检验两个正态总体

![](/assets/images/Pasted image 20250611222012.png)

现在就是两个问题：

1. 要检验两个样本的均值相不相等；
2. 要检验两个样本的方差相不相等。
步骤跟上面还是差不多，均值的话 $\sigma$ 已知就用 U 检验，也叫 Z 检验；$\sigma$ 未知就用 T 检验；对于方差，这个跟上面不同了，要用 F 检验。就是那个 F 分布

![](/assets/images/Pasted image 20250611222241.png)

这里面直接让 $\mu_1-\mu_2=0$ ，也就是消去分子里的括号项，即可。

![](/assets/images/Pasted image 20250611222419.png)

这个能推，但是推过之后，发现有点恶心 略去

还有那个标题少打了等号

下面这个是 F 分布：

![](/assets/images/Pasted image 20250611222723.png)

第四步就是确定统计量的值，与临界值比较，下结论，结束

# Chap 9

不会 但是要考 我嘞个

反转了 Gemini 2.5 pro preview 用不到 10 分钟给我教会了 反观那个课件 我寻思着故意把课件写得很恶心，让人看不懂，搞些什么防自学机制是什么好事吗？也没增加上你课的人啊，平常都是来 10 多个人啊 亏你还用理教 107 那么大的教室

不管了。先看看内容

## ANOVA

没问题，别着急！方差分析 (ANOVA) 听起来复杂，但它的核心思想非常直观。我结合你的课件，用最容易理解的方式给你讲明白，保证你明天考试有底气。

我们就把方差分析想象成一场“破案”，目标是判断一个“嫌疑人”（我们称之为**因素**）到底是不是“真凶”（对结果产生了显著影响）。

---

### 1. 故事背景：方差分析是做什么的？

想象一下你的课件里那个例子：我们要比较 A、B、C 三种不同类型灯管的寿命。

* **问题**: 这三种灯管的平均寿命真的有区别吗？还是说，我们抽样测出来的寿命差异，仅仅是运气不好（抽到了一些本身就耐用或不耐用的灯管）造成的？

* **传统方法**: 如果只有两种灯管，我们可以用 t-检验。但现在有三种（或更多），两两比较（A-B, A-C, B-C）会增加犯错的概率。

* **ANOVA登场**: 方差分析就是专门用来一次性比较 **三个或更多** 组（你的课件里叫“水平”）的平均值，判断它们之间是否存在显著差异的工具。

---

### 2. 核心思想：比较两种“差异”

这是方差分析最关键、最核心的思想！它不直接比较平均值，而是通过比较“方差”（也就是数据的波动/差异程度）来解决问题。

它关注两种差异：

1.  **组间差异 (Between-group variation)**: 指的是 **不同** 种类的灯管（A组、B组、C组）的平均寿命之间的差异。如果这个差异很大，说明“灯管类型”这个因素 **可能** 真的造成了寿命不同。这在你的课件里叫做 **效应平方和 $S_A$**。我们可以把它理解为“嫌疑人”（因素）可能造成的差异。

2.  **组内差异 (Within-group variation)**: 指的是 **同一种** 灯管内部，各个灯管寿命的差异。比如，A类灯管里抽了8个，这8个灯管的寿命也不可能完全一样。这种差异源于各种随机因素（比如生产中的微小误差），是“偶然”的。这在你的课件里叫做 **误差平方和 $S_E$**。我们可以把它理解为“背景噪音”或者“随机产生的差异”。

**破案的逻辑来了**：

> 如果 **组间差异** 明显大于 **组内差异**，就好像“嫌疑人”造成的动静远远盖过了“背景噪音”。我们就有理由相信，这个“嫌疑人”（因素）就是“真凶”，即不同类型的灯管寿命确实有显著差异。

> 反之，如果组间差异和组内差异差不多大，那说明组与组之间的这点差别很可能只是随机的“背景噪音”而已，我们不能判定“嫌疑人”有罪。

---

### 3. 数学“黑话”翻译（对照你的课件）

现在我们把上面那个逻辑翻译成数学语言，你的课件就好懂了。

#### 平方和分解 (Sum of Squares Decomposition)

这是方差分析的数学基础。它把整体数据的总波动，拆分成了上面说的两部分。

* **总偏差平方和 $S_T$ (Total Sum of Squares)**: 代表所有数据（24个灯管）的总差异有多大。

$$
 S_T = \sum_{j=1}^{r} \sum_{i=1}^{n_j} (X_{ij} - \bar{X})^2
$$

    * $X_{ij}$ 是第j组的第i个数据点（比如B类灯管的第3个灯管寿命）。

    * $\bar{X}$ 是所有24个灯管的**总平均**寿命。

    * 这个公式就是在算每个灯管寿命和总平均寿命的差距的平方和。

* **效应平方和 $S_A$ (Sum of Squares due to Factor A)**: 这就是我们说的 **组间差异**。

$$
 S_A = \sum_{j=1}^{r} n_j (\bar{X}_{\cdot j} - \bar{X})^2
$$

    * $\bar{X}_{\cdot j}$ 是第j组的**组内平均**寿命（比如A类灯管的平均寿命）。

    * $n_j$ 是第j组的样本数量（这里是8）。

    * 这个公式计算的是“每个组的平均值”与“总平均值”的差异。差异越大，说明组和组之间差别越大。

* **误差平方和 $S_E$ (Sum of Squares of Error)**: 这就是我们说的 **组内差异**。

$$
 S_E = \sum_{j=1}^{r} \sum_{i=1}^{n_j} (X_{ij} - \bar{X}_{\cdot j})^2
$$

    * 这个公式计算的是“一个组内，每个数据点”与“该组的平均值”的差异。它衡量的是一个组内部的波动有多大。

**最重要的公式**：

你的课件证明了这个核心关系：

$$
 S_T = S_A + S_E
$$

**翻译**: **总差异 = 组间差异 + 组内差异**。完美对应我们上面的逻辑！

#### F检验 (The F-Test)

我们怎么判断“组间差异”是否“明显大于”组内差异呢？就要用到F检验。

1.  **构造检验统计量F**:

    为了消除样本数量的影响，我们不能直接用 $S_A$ 除以 $S_E$。需要用它们各自的“均方”（Mean Square, MS）来计算。均方就是把平方和除以它的自由度（degrees of freedom, df）。

    * 组间均方: $MS_A = \frac{S_A}{r-1}$ (自由度是 组数-1)

    * 组内均方: $MS_E = \frac{S_E}{n-r}$ (自由度是 总样本数-组数)

    于是，我们的F统计量就是：

$$
F = \frac{MS_A}{MS_E} = \frac{S_A / (r-1)}{S_E / (n-r)}
$$

    你看，这不就是 **(处理过的组间差异) / (处理过的组内差异)** 吗！

2.  **如何决策？**

    * **零假设 $H_0$**: 三种灯管的平均寿命 **没有** 显著差异 ($\mu_1 = \mu_2 = \mu_3$)。

    * **备择假设 $H_1$**: 三种灯管的平均寿命 **不完全相等** (至少有一个和别的不同)。

    我们算出一个F值后，将它与一个“F分布表”中的**临界值** $F_\alpha(r-1, n-r)$ 进行比较（或者直接看p值）。

    * 如果算出来的 **F值 > 临界值** (或者 p值 < $\alpha$，比如0.05)，说明F值大到了不太可能是偶然发生的程度。我们就 **拒绝$H_0$**，得出结论：灯管类型对寿命**有显著影响**。

    * 如果算出来的 **F值 ≤ 临界值** (或者 p值 ≥ $\alpha$)，说明组间差异不够突出，我们 **不能拒绝$H_0$**，得出结论：**没有足够证据**表明灯管类型对寿命有影响。

---

### 4. 考试实战：方差分析表

考试时，你很可能会被要求填一个方差分析表。这个表就是把上述计算过程整理起来，一目了然。

| 差异来源 (Source)     | 平方和 (SS)    | 自由度 (df)  | 均方 (MS)           | F值          |
| :---------------- | :---------- | :-------- | :---------------- | :---------- |
| **组间 (Factor A)** | $S_A$       | $r-1$     | $MS_A = S_A/df_A$ | $MS_A/MS_E$ |
| **组内/误差 (Error)** | $S_E$       | $n-r$     | $MS_E = S_E/df_E$ |             |
| **总计 (Total)**    | $S_T$       | $n-1$     |                   |             |

**解题步骤**:

1.  看题目，确定因素（灯管类型）、水平（A, B, C，所以r=3）、总样本数n。

2.  根据给的数据，计算 $S_T, S_A, S_E$。(考试时通常会简化，比如直接给你其中两个)

3.  填表：

    * 填入平方和SS。

    * 计算自由度df：$df_A = r-1$, $df_E = n-r$, $df_T = n-1$。检查一下：$df_A + df_E = df_T$。

    * 计算均方MS：$MS = SS/df$。

    * 计算F值：$F = MS_A / MS_E$。

4.  查表或根据给定的p值，与显著性水平$\alpha$比较，做出统计决策（拒绝或不拒绝$H_0$）。

5.  用通俗的语言写出你的结论。

---

### 总结一下

* **目的**：一次性比较多个组的平均值是否有差异。

* **思路**：比较 **组间差异** 和 **组内差异**。

* **工具**：**F统计量**，它是 `组间差异/组内差异` 的一个量度。

* **结论**：如果F值很大，意味着组间差异远大于随机的组内差异，我们就认为这个因素是显著的。

希望这个讲解能让你对ANOVA有一个清晰的框架。别怕那些公式，理解了“组间差异 vs 组内差异”这个核心故事，公式只是实现这个故事的工具而已。
