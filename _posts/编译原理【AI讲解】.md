---
tags:
  - 计算机
  - 专业课
  - AI
---

> 输入的Prompt：
> 你好，这是我们的教学课件，但是我这节课没有听，请你对照着课件内容，帮我由浅入深、清晰易懂地讲一遍。 你可以采取以下的方法：对于课件的每一页，需要说明的内容，指出它所在的页码，然后对课件中的内容进行阐述。 PDF 和另外的文件内容相同，你可以互相对照。

# Lect 2 词法分析之一
好的，没问题。这门课是计算机科学的核心课程，词法分析是编译器的第一个重要步骤，理解它至关重要。别担心，我会一页一页地带你过一遍，力求讲得清晰易懂。

我们就从头开始吧。

---

### **第 1-2 页：课程介绍与内容提要**

在 **第 1 页**，我们能看到这节课的主题是 **《编译原理》的第 3 章：词法分析（第一部分）**。

在 **第 2 页** 的内容提要中，列出了整个“词法分析”章节会涉及的所有知识点。这节课作为第一部分，主要会集中在前两个主题：

1. **词法分析器的作用**：它在编译器里是干什么的？
    
2. **词法单元的规约**：我们如何用一种精确、无歧义的语言来描述我们要识别的东西？这里会引入一些非常重要的理论工具，比如“正则表达式”。
    

后面的内容，如有限自动机（Finite Automata）、LEX 工具等，会在后续课程中讲解。今天我们先打好基础。

---

### **第 3 页：词法分析器的作用**

这一页用一张图非常直观地展示了词法分析器（Lexical Analyzer）在编译器中的位置和角色。你可以把它想象成编译器的“眼睛”和“预处理器”。

- **输入**：词法分析器读取的是最原始的 **源程序**，也就是你写的代码，在它看来，这只是一长串的字符流，比如 `w, h, i, l, e, , i, , <, >, ...`。
    
- **输出**：它会把这些零散的字符“打包”成一个个有意义的、原子性的单元，称为 **“词法单元”（Token）**。比如，它会把 `w, h, i, l, e` 这五个字符识别成一个整体，标记为关键字 `while`。
    
- **核心任务**：
    
    1. **识别与分组**：将字符流切分成一个个 Token。
        
    2. **过滤无用信息**：像代码中的空格、换行、注释等，对程序的逻辑没有影响，词法分析器会直接“扔掉”它们。
        
    3. **与后续阶段交互**：
        
        - 它被 **语法分析器 (Parser)** 调用。你可以想象语法分析器是“大脑”，当它需要下一个“单词”来分析句子结构时，就会向词法分析器发出指令：“取下一个 Token”。
            
        - 它会把识别出的信息（比如变量名）登记到 **符号表 (Symbol Table)** 中，方便后续阶段查询。
            

**简单比喻**：就像你读一篇英文文章，你不会一个字母一个字母地看，而是自动地把字母组合成单词（比如 "compiler"），并忽略单词间的空格和标点。词法分析器做的就是类似的工作。

---

### **第 4 页：词法单元、模式、词素（Token, Pattern, Lexeme）**

这是三个非常重要且容易混淆的概念，理解它们的区别是学好词法分析的关键。

- **词素 (Lexeme)**：是源程序中实实在在的 **字符序列**。它就是你在代码里写下的那个东西。例如，在代码 `count = 10;` 中，`count`、`=`、`10`、`;` 都是词素。
    
- **模式 (Pattern)**：是描述一类词素的 **规则**。这个规则告诉词法分析器“长成什么样的词素属于这一类”。例如：
    
    - C 语言标识符（变量名）的模式可以是：“以字母或下划线开头，后跟零个或多个字母、数字或下划线”。
        
    - 整数常量的模式可以是：“一个或多个数字组成的序列”。
        
- **词法单元 (Token)**：是一个抽象的符号，代表了一类词素。通常表示为一个二元组：**`<词法单元名, 属性值>`**。
    
    - **词法单元名 (Token Name)**：是给语法分析器看的，告诉它“这是个什么类型的东西”。比如 `id` (标识符), `number` (数字), `keyword` (关键字)。
        
    - **属性值 (Attribute Value)**：是给更后面的语义分析阶段看的，它提供了这个词素的具体信息。
        
        - 比如，对于词素 `count`，它的 Token 可能是 `<id, "指向 count 在符号表中的条目">`。
            
        - 对于词素 `10`，它的 Token 可能是 `<number, 10>`。
            
        - 对于关键字 `while`，它没有特殊属性，Token 可以是 `<keyword_while, ->` 或者简化为 `<while>`。
            

**总结一下关系**：词法分析器根据 **模式 (Pattern)** 去匹配源程序中的 **词素 (Lexeme)**，匹配成功后，生成一个对应的 **词法单元 (Token)** 交给下一阶段。

---

### **第 5-9 页：词法分析的具体例子**

这几页通过实例来巩固前面的概念。

- **第 5 页**：展示了一段简单的代码 `while i <> j do ...`，然后下面列出了词法分析器识别出的 **词素** 序列。这就是最直观的“切词”过程。
    
- **第 6 页**：总结了词法分析器的功能，除了识别 Token，还包括：
    
    - 将数字字符串（如 "256"）转换成真正的数值（二进制的 256）。
        
    - 记录 Token 的位置（行号、列号），这样报错时才能精确定位。
        
    - 宏扩展等预处理工作。
        
- **第 7 页**：列举了常见的 Token 类别，比如关键字、标识符、常数、运算符、分界符。
    
- **第 8 页**：再次强调了 Token 的输出格式是 `<类别编码, 属性值>`，并且解释了这两部分分别给谁用（类别给语法分析，属性给语义分析）。
    
- **第 9 页**：将第 5 页的代码转换成了最终的 Token 序列。
    
    - `while` 变成了 `<while, —>`
        
    - `i` 变成了 `<id, 指向 i 的符号表入口的指针>`
        
    - `<>` 变成了 `<relational-op, NE>` (NE 代表 Not Equal)
        
    - 这个过程非常清晰地展示了从代码文本到抽象 Token 的转换。
        

---

### **第 10-12 页：词法分析的难点——向前看 (Lookahead)**

这几页的例子说明，词法分析并非总是从左到右扫一遍那么简单，有时需要“多看几个字符”才能做出决定。

- **第 10 页 (FORTRAN 例子)**：
    
    - `DO 5 I = 1,25` 是一个循环语句。
        
    - `DO 5 I = 1.25` 是一个赋值语句（把 1.25 赋给变量 `DO5I`，因为 FORTRAN 忽略空格）。
        
    - 当词法分析器读到 `DO 5 I = 1` 时，它完全无法判断这是循环还是赋值，直到它看到后面的逗号 `,` 还是小数点 `.`。这个“向后多看几眼”的动作就叫 **向前看 (Lookahead)**。
        
- **第 11 页 (PL/1 例子)**：
    
    - PL/1 语言的关键字不是保留字，意味着 `IF`, `THEN`, `ELSE` 既可以是关键字，也可以是变量名。
        
    - 在 `IF ELSE THEN THEN = ELSE;` 这句神仙代码里，词法分析器需要结合语法规则，可能需要看得非常远才能确定每个词到底是什么身份。这说明有时需要 **无限长的向前看**。
        
- **第 12 页 (C++ 例子)**：
    
    - C++ 的 `>>` 既可以表示右移运算符或流输入运算符，也可以是模板嵌套的结束符，比如 `Foo<Bar<Barr>>`。
        
    - 早期的 C++ 编译器会把 `>>` 贪心地优先识别成一个右移运算符，导致模板代码编译失败。后来 C++ 标准规定，在这种情况下，要将其拆成两个 `>`。这说明词法分析有时还需要 **上下文信息** 才能正确工作。
        

这些例子告诉我们，虽然词法分析看起来简单，但在处理复杂语言时，也会遇到挑战。

---

### **第 13-15 页：词法分析的实现与规约**

- **第 13 页**：讨论了词法分析器的两种实现方式：
    
    1. **单独一遍扫描 (Pass)**：先把整个源文件全部转换成 Token 文件，然后语法分析器再读取这个 Token 文件。设计简单，但不常用。
        
    2. **作为子程序**：语法分析器需要 Token 时就调用一个函数（如 `getToken()`），词法分析器就去源文件中读一小段，生成一个 Token 再返回。这是最常见、最高效的方式。
        
- **第 14-15 页**：开启了本节课的第二个核心话题——**词法单元的规约 (Specification)**。
    
    - “规约”就是“精确地定义和描述”。我们前面用自然语言描述了模式（比如“字母开头的字母数字串”），但自然语言有歧义，不够形式化。
        
    - 为了让计算机精确地理解和执行，我们需要一个数学工具来描述这些模式。这个强大的工具就是 **正则表达式 (Regular Expression)**。
        

---

### **第 16-25 页：正则表达式的基础——串和语言**

这部分是在为正则表达式做铺垫，定义了一些基础的数学概念。

- **第 16 页 (字母表, Alphabet)**：一个有限的符号集合，用 Σ\Sigma 表示。例如，二进制字母表 Σ={0,1}\Sigma = \{0, 1\}，ASCII 字母表等。
    
- **第 17-21 页 (符号串, String)**：由字母表中的符号组成的有限序列。
    
    - **空串 (ϵ\epsilon)**：长度为 0 的特殊串。
        
    - **术语**：前缀、后缀、子串等（这些概念和我们平时理解的字符串操作一样）。
        
    - **运算**：连接 (concatenation) 和方幂 (exponentiation)。`x` 连接 `y` 记作 `xy`，`x` 的 2 次方是 `xx`。
        
- **第 22-25 页 (语言, Language)**：一个字母表上 **所有可能的符号串的集合** 的一个 **子集**。
    
    - 语言本质上就是一个 **字符串的集合**。它可以是有限集（如 C 语言所有关键字的集合），也可以是无限集（如 C 语言所有合法标识符的集合）。
        
    - **语言的运算**：
        
        - **并 (Union)**: L∪ML \cup M，包含 L 或 M 中所有串的集合。
            
        - **连接 (Concatenation)**: LMLM，从 L 中取一个串 s，从 M 中取一个串 t，拼接成 st，所有可能的 st 组成的集合。
            
        - **闭包 (Closure)**：这是最重要的运算！
            
            - **克林闭包 (Kleene Closure, L*)**：表示“从 L 中取 **零个或多个** 串进行连接”得到的所有串的集合。L∗=L0∪L1∪L2∪…L^* = L^0 \cup L^1 \cup L^2 \cup \dots。注意，L0={ϵ}L^0 = \{\epsilon\}，所以 L∗L^* **永远包含空串** ϵ\epsilon。
                
            - **正闭包 (Positive Closure, L+)**：表示“从 L 中取 **一个或多个** 串进行连接”得到的所有串的集合。L+=L1∪L2∪…L^+ = L^1 \cup L^2 \cup \dots。它就是 L∗L^* 中去掉了 ϵ\epsilon 的情况（除非 ϵ\epsilon 本身就在 L 里）。
                

---

### **第 26-29 页：正则表达式 (Regular Expression)**

现在，我们终于来到了定义模式的工具——正则表达式。

- **第 26 页**：给出了正则表达式的递归定义。
    
    1. **基本情况 (Base Cases)**：
        
        - ϵ\epsilon 是一个正则表达式，它表示的语言是 `{\(\epsilon\)}`（只包含空串的集合）。
            
        - 字母表 Σ\Sigma 中的任何一个符号 `a` 都是一个正则表达式，它表示的语言是 `{a}`。
            
    2. **归纳步骤 (Inductive Steps)**：如果 `r` 和 `s` 都已经是正则表达式，那么：
        
        - **选择 (Alternation)**：`(r)|(s)` 也是正则表达式，表示的语言是 `L(r) ∪ L(s)` (或)。
            
        - **连接 (Concatenation)**：`(r)(s)` 也是正则表达式，表示的语言是 `L(r)L(s)` (连接)。
            
        - **克林闭包 (Kleene Closure)**：`(r)*` 也是正则表达式，表示的语言是 `(L(r))*` (零个或多个)。
            
- **第 27 页**：通过例子来理解这些定义。
    
    - `a|b`：匹配 `a` 或者 `b`。
        
    - `(a|b)(a|b)`：匹配 `aa`, `ab`, `ba`, `bb`。
        
    - `a*`：匹配 `\(\epsilon\)`, `a`, `aa`, `aaa`, ...
        
    - `(a|b)*`：匹配由 `a` 和 `b` 组成的任意长度的串，包括空串。
        
    - **C 语言标识符** 的模式就可以用正则表达式描述：`(letter_)(letter_ | digit)*` （这里 `letter_` 和 `digit` 是对一类字符的简写）。
        
- **第 28 页 (Quiz)**：这个问题是 `(0 | 1 | \(\epsilon\))(0 | 1 | \(\epsilon\))(0 | 1 | \(\epsilon\))(0 | 1 | \(\epsilon\))` 匹配多少个不同的字符串？
    
    - 我们来分析一下：`(0 | 1 | \(\epsilon\))` 可以匹配 3 种东西：`0`，`1`，或者什么都没有（空串）。
        
    - 整个表达式是四个这样的部分连接起来。根据乘法原理，总共可以产生的不同字符串数量是 3×3×3×3=34=813 \times 3 \times 3 \times 3 = 3^4 = 81。所以答案是 f) 81。
        
- **第 29 页**：列出了正则表达式的一些代数性质，比如交换律、结合律、分配律等。这些性质在理论分析和工具实现中很有用。
    

---

### **第 30-32 页：正则定义与扩展**

正则表达式写起来可能很长很复杂，所以引入了一些语法糖来简化书写。

- **第 30-31 页 (正则定义, Regular Definition)**：这相当于给正则表达式起一个别名。
    
    - 我们可以先定义：
        
        - `digit` →\rightarrow `0 | 1 | ... | 9`
            
        - `letter_` →\rightarrow `A | ... | Z | a | ... | z | _`
            
    - 然后就可以用这些别名来定义更复杂的模式：
        
        - `id` →\rightarrow `letter_ ( letter_ | digit )*`
            
    - 这样做极大地增强了可读性。
        
- **第 32 页 (正则表达式的扩展)**：引入了更多方便的运算符，它们本质上都可以用基础的 `|`, `·`, `*` 来表示。
    
    - **正闭包 `+`**：`r+` 表示“一次或多次”，等价于 `rr*`。
        
    - **可选 `?`**：`r?` 表示“零次或一次”，等价于 `r | \(\epsilon\)`。
        
    - **字符类 `[...]`**：`[abc]` 等价于 `a|b|c`，`[0-9]` 等价于 `0|1|...|9`。
        
    - 使用这些扩展后，C 语言标识符的定义可以写得非常简洁：`[A-Za-z_][A-Za-z_0-9]*`。
        

---

### **第 33-36 页：Lex 工具中的正则表达式**

这几页介绍了在实际的词法分析器生成工具 Lex (或 Flex) 中，正则表达式的语法和一些特殊功能。

- **第 33 页**：是一个表格，列出了 Lex 中常用的元字符及其含义。
    
    - `.` (点)：匹配除换行符外的任意单个字符。
        
    - `^` (尖角号)：匹配一行的开始。
        
    - `$` (美元符)：匹配一行的结尾。
        
    - `"s"` (双引号)：匹配字符串 s 的字面值，忽略其中的特殊字符。
        
- **第 34 页**：解释了一些特殊字符的处理。
    
    - 如果要匹配 `*`、`.`、`?` 这些有特殊含义的字符本身，需要用 `\` 来转义，例如 `\*` 匹配一个星号。
        
    - `[^s]` 表示 **补集字符类**，即匹配任何 **不在** s 中的字符。例如 `[^a-z]` 匹配任何非小写字母的字符。
        
- **第 35 页**：讨论了 `^` 和 `$`。
    
    - `^` 的双重含义：在正则表达式开头，表示行首；在字符类 `[]` 的开头，表示补集。Lex 根据上下文可以区分。
        
    - `^[^aeiou]*$` 这个例子匹配的是：从行首到行尾，整行都不包含任何元音字母的字符串。
        
    - 最后的问题很有趣：`^` 和 `$` 能否被等价替换？通常不能，因为它们匹配的是 **位置**（行首/行尾），而不是具体的字符。
        
- **第 36 页 (例题)**：这是一个比较复杂的正则定义问题：“所有相邻数字都不相同的非空数字串”。PPT 上给出的解答思路是一种递归或递推的构造方式，从最简单的情况（如 `no_0-8` -> `9`）开始，一步步向上构造更复杂的定义。这展示了正则定义的威力。
    

---

### **第 37 页：作业**

最后这一页是留的课后作业，让你通过练习来巩固本节课学习的正则表达式和正则定义的知识。

---

### **总结**

好了，以上就是对这份课件的逐页讲解。我帮你总结一下今天的核心要点：

1. **词法分析器**是编译器的“前锋”，负责将原始的字符代码流转换成一系列规范化的 **Token**。
    
2. 为了精确描述如何识别 Token，我们引入了形式化工具——**正则表达式**。
    
3. 你需要牢固掌握正则表达式的三个基本运算：**选择 `|`、连接 `·`、闭包 `*`**，以及它们的扩展形式 `+`, `?`, `[]`。
    
4. **正则定义** 是一种给正则表达式起别名的方法，能让复杂的模式描述变得清晰易读。
    

希望这次的讲解能帮助你补上这节课的内容。如果对某个具体知识点还有疑问，随时可以再问我！

### Q&A
![[Pasted image 20251010131103.png]]

> 25页那三个问题的答案是？

当然！这三个问题是理解“语言运算”这个概念的绝佳练习。它们考察的是你是否真正理解了语言（即字符串集合）的**连接 (concatenation)** 运算的本质。

我们来逐一分析：

---

#### **1. `LM = ML?`**

**答案：一般情况下不相等。**

**原因：**  
这个问题的核心是：语言的连接运算是否满足 **交换律**？

- **回顾定义**：语言 `LM` 是这样构成的：从 `L` 中任取一个字符串 `s`，从 `M` 中任取一个字符串 `t`，然后把它们拼接成 `st`。`LM` 包含所有这样拼接出来的结果。
    
- 同理，`ML` 是先从 `M` 中取 `t`，再从 `L` 中取 `s`，拼接成 `ts`。
    

我们知道，对于普通的字符串，拼接顺序是至关重要的。比如，字符串 `"ab"` 和字符串 `"ba"` 是完全不同的。既然底层的字符串拼接不满足交换律，那么建立在其上的语言连接运算通常也不满足。

**举一个最简单的反例：**

- 假设语言 `L = {"a"}`
    
- 假设语言 `M = {"b"}`
    

那么：

- `LM = { "ab" }` (从 L 取 "a"，从 M 取 "b"，拼接)
    
- `ML = { "ba" }` (从 M 取 "b"，从 L 取 "a"，拼接)
    

显然，`{ "ab" }` 和 `{ "ba" }` 是两个不同的集合，所以 `LM ≠ ML`。

---

#### **2. `{ε}M = M{ε} ?`**

**答案：相等。** 它们不仅相等，而且都等于 `M` 本身。

**原因：**  
这个问题考察的是 **空串 `ε`** 在字符串连接中的特殊作用。空串 `ε` 是连接运算的 **单位元 (Identity Element)**，就像数字 1 是乘法的单位元一样 (`1 * x = x * 1 = x`)。

- **分析左边 `{ε}M`**：
    
    - 根据定义，我们要从 `{ε}` 中取一个字符串 `s`，从 `M` 中取一个字符串 `t`，拼接成 `st`。
        
    - 但 `{ε}` 这个集合里只有一个成员，就是空串 `ε`。所以 `s` 只能是 `ε`。
        
    - 因此，所有拼接结果都是 `εt` 的形式。
        
    - 任何字符串 `t` 和空串 `ε` 拼接，结果还是 `t` 本身 (`εt = t`)。
        
    - 所以，`{ε}M` 的结果就是把 `M` 里的所有字符串都拿出来，原封不动地组成一个新的集合。这个新集合就是 `M` 本身。
        
- **分析右边 `M{ε}`**：
    
    - 同理，拼接的结果都是 `tε` 的形式。
        
    - 由于 `tε = t`，所以 `M{ε}` 的结果也是 `M` 本身。
        

**结论：`{ε}M = M = M{ε}`，所以它们是相等的。**

---

#### **3. `{ }M = M{ } ?`**

**答案：相等。** 它们不仅相等，而且都等于 **空集 `{ }`**。

**原因：**  
这个问题非常关键，它考察的是你是否能区分 **空集 `{ }`** 和 **包含空串的集合 `{ε}`**。

- `{ε}` 是一个**非空**集合，它里面有**一个**成员：空串 `ε`。
    
- `{ }` (也写作 `∅`) 是一个**空**集合，它里面**没有任何**成员。
    

现在我们来分析：

- **分析左边 `{ }M`**：
    
    - 根据定义，我们要从 `{ }` 中取一个字符串 `s`，从 `M` 中取一个字符串 `t`，拼接成 `st`。
        
    - 但是，**你根本无法从空集 `{ }` 中取出任何字符串 `s`！** 因为它里面什么都没有。
        
    - 既然一个拼接对 `(s, t)` 都无法构成，那么最终的结果集合里自然也什么都没有。
        
    - 所以，`{ }M = { }`。
        
- **分析右边 `M{ }`**：
    
    - 同理，你无法从 `{ }` 中取出任何字符串 `t`。
        
    - 所以，`M{ } = { }`。
        

**结论：`{ }M = { } = M{ }`，所以它们是相等的。**

你可以把空集 `{ }` 看作是语言连接运算中的 **零元 (Zero Element)**，就像数字 0 在乘法中的作用一样 (`0 * x = x * 0 = 0`)。任何语言和空集进行连接运算，结果都是空集。

> 前缀是否包含空串？

是的，**前缀一定包含空串（ε）**。

这是一个非常重要的约定，无论是在编译原理、形式语言还是字符串算法中都成立。

我们回顾一下课件 **第 19 页** 对前缀的定义：

> **前缀 (prefix)**：移走 x 尾部的 **零个或多个** 连续的符号

这里的关键就在于 **“零个”** 这个词。

让我们用一个例子 `x = banana` (课件第 20 页) 来解释：

1. **移走 0 个符号**：
    
    - 从 `banana` 的尾部移走 0 个符号，什么也没动，剩下的就是 `banana` 本身。所以 `banana` 是它自己的前缀。
        
2. **移走 1 个符号**：
    
    - 从尾部移走 `a`，剩下 `banan`。所以 `banan` 是前缀。
        
3. **移走 2 个符号**：
    
    - 从尾部移走 `na`，剩下 `bana`。所以 `bana` 是前缀。
        

...依此类推...

6. **移走 6 个符号**：
    
    - 从尾部移走整个 `banana`，剩下的就是 **空串 ε**。所以 **ε 是 `banana` 的前缀**。
        

**总结一下：**

对于任何一个非空字符串 `s`，它的前缀集合总是包含：

1. **空串 ε** (通过移走 `s` 的所有字符得到)。
    
2. **字符串 `s` 本身** (通过移走 0 个字符得到)。
    
3. 所有从开头截取的、长度介于 `ε` 和 `s` 之间的子串。
    

**同样地，后缀也包含空串 ε**。  
因为后缀的定义是“移走 x 头部的零个或多个连续的符号”。如果你把一个字符串的所有符号都从头部移走，剩下的就是空串 ε。



# Lect 3 词法分析之二

好的，同学你好。这门课没去听讲确实会有点跟不上，不过别担心，这套课件的核心内容非常清晰。我们一起来把它过一遍，我会尽量用好懂的方式给你讲清楚。

首先，我用一句话告诉你这节课的核心目的：

**这节课是为了教会你，如何将一长串计算机程序代码，准确地“切分”成一个个有意义的基础单词（比如关键字`if`、运算符`+`、变量名`my_var`等），这是所有编译工作的第一步。课程将带你从理论（正则表达式、有限自动机）走向实践（状态转换图、自动化工具Lex），让你彻底明白词法分析器的工作原理和构建方法。**

---

接下来，我们一页一页地来看课件内容。

#### **第2页：上节内容回顾**

这一页是复习。它提醒我们，**词法分析器 (Lexical Analyzer)** 的工作，就像一个“单词识别器”。它负责读取源代码，然后把识别出的“单词”——也就是 **词法单元 (Token)** ——一个一个地交给下一阶段的 **语法分析器 (Parser)**。同时，它还会把变量名、函数名这类信息登记在 **符号表 (Symbol Table)** 中。为了精确地描述我们要找什么样的“单词”，我们使用了**正则表达式**。

#### **第3页：Questions (问题回顾)**

这一页通过几个问题，来检验你对正则表达式的掌握程度，这几个问题层层递进，很有意思。

- **问题1：写一个正则表达式，表示所有能被5整除的十进制数。**
    
    - **答案：(0∣1∣…∣9)∗(0∣5)(0|1|…|9)*(0|5)**
        
    - **讲解：** 一个十进制数能不能被5整除，只看它的最后一位是不是 `0` 或 `5`。这个正则表达式的意思是：前面可以是任意数字（(0∣1∣…∣9)∗(0|1|…|9)*），但最后一位必须是 `0` 或 `5`（(0∣5)(0|5)）。
        
- **问题2：在问题1的基础上，要求不包含前导0。**
    
    - **答案：0∣5∣(1∣2∣…∣9)(0∣1∣…∣9)∗(0∣5)0 | 5 | (1|2|…|9)(0|1|…|9)*(0|5)**
        
    - **讲解：** 这个问题更严格了。我们需要分情况讨论：
        
        1. 如果这个数就是 `0` 或 `5` 本身，是允许的。
            
        2. 对于多位数，第一位不能是 `0`（(1∣2∣…∣9)(1|2|…|9)），后面可以跟任意数字（(0∣1∣…∣9)∗(0|1|…|9)*），但最后一位仍然必须是 `0` 或 `5`。所以把这几种情况用 `|`（表示“或”）连接起来就是最终答案。
            
- **问题3：写一个正则表达式，表示所有能被5整除的二进制数。**
    
    - **答案：(0∣1(10)∗(0∣11)(01∗01∣01∗00(10)∗(0∣11))∗1)∗(0|1(10)*(0|11)(01*01|01*00(10)*(0|11))*1)***
        
    - **讲解：** 这个问题非常难，它的答案看起来像天书。你不需要现在就理解这个表达式的每一部分，老师把它放在这里是为了告诉你：虽然正则表达式很强大，但对于某些复杂规则，写出来的表达式会变得极其复杂。这也引出了本节课的重点：我们有比“人肉手写”复杂正则表达式更好的方法来做词法分析，那就是**有限自动机**。这个复杂的例子其实就是一个有限自动机的等价表达。
        

#### **第5页：如何识别词法单元？**

这一页是本节课的“总纲”，告诉我们实现词法分析器的两种主流方法。

1. **手动构造：** 就像自己画一张“寻宝图”一样，我们通过画 **状态转换图 (Transition Diagram)** 来手动设计和编写词法分析程序。
    
2. **自动生成：** 使用一个叫 **LEX** 的工具，我们只需要提供描述“单词”长什么样的**正则表达式**，它就能自动帮我们生成一个高效的词法分析器。
    

而LEX工具的内部魔法就是下面这个流程，这也是本节课最重要的知识链条：  
正则表达式→NFA→DFA→minDFA→词法分析器代码\text{正则表达式} \rightarrow \text{NFA} \rightarrow \text{DFA} \rightarrow \text{minDFA} \rightarrow \text{词法分析器代码}

- **NFA/DFA** 就是“有限自动机”，后面会详细讲。简单说，它们就是状态转换图的数学形式。
    

#### **第6页：状态转换图**

这一页解释了什么是状态转换图，它是我们手动构造词法分析器的蓝图。

- **状态 (State)**：一个圆圈，代表“目前为止，我识别到了什么”。比如，我刚读了一个`<`，就进入了“可能是一个小于号或者小于等于号”的状态。
    
- **边 (Edge)**：连接状态的箭头，表示“在当前状态下，如果新来一个这个字符，我就进入下一个状态”。
    
- **开始状态 (Start State)**：整个识别过程的起点。
    
- **接受状态 / 最终状态 (Accepting State)**：双层圆圈或者有特殊标记的圆圈。一旦进入这个状态，就表示“恭喜，我们成功识别出了一个完整的词法单元！”
    
- **带 `*` 的接受状态**：这个非常重要！它表示为了识别当前这个词法单元，我们“多读”了一个不属于它的字符。在识别结束后，需要把这个多读的字符“吐”回去（回退一格），留给下一次识别用。
    

#### **第7-10页：状态转换图实例**

这几页用非常直观的图展示了如何为不同类型的词法单元设计转换图。

- **第7页 (关系算符)**：这张图完美展示了如何识别 `<`, `<=`, `=`, `<>`, `>`, `>=`。
    
    - 比如从`开始`状态，读到`<`，进入状态1。
        
    - 在状态1，如果下一个是`=`，就进入状态2，成功识别出`<=`。
        
    - 如果在状态1，下一个是`>`，就进入状态4，识别出`<>`。
        
    - 如果在状态1，下一个是`other`（任何其他字符），就进入带`*`的状态3。这意味着我们已经识别完了`<`，那个`other`字符是多读的，要退还回去。
        
- **第8页 (标识符和保留字)**：标识符（变量名等）通常是“字母开头，后跟字母或数字”。这张图描述了这个规则。`getToken()`和`installId()`是识别成功后要执行的动作（比如查一下这个标识符是不是一个关键字，然后存到符号表里）。
    
- **第9页 (无符号数)**：这张图看起来复杂，但它其实就是把数字的各种形式（整数`123`，小数`12.34`，科学计数法`1.2E-5`）用状态机画了出来。你可以随便想一个数字，然后顺着箭头走一遍，看看是否能走到最终状态。
    
- **第10页 (空白)**：这个最简单，就是吃掉一个或多个空格、制表符、换行符。
    

#### **第11页：手动编写词法分析程序**

这一页展示了第7页的状态图如何翻译成真实的C语言代码。你会发现，它就是一个`while(1)`循环里包一个`switch(state)`。程序通过一个`state`变量来记住当前在哪个状态，根据读入的`char`来决定`state`如何变化，这和我们在图上用手指顺着箭头走是完全一样的。

#### **第12-13页：词法单元的自动识别**

从这里开始，课程的重心从“手动”转向“自动”。自动识别的过程如下：

1. 为每一类词法单元（如标识符、数字、运算符）都写一个正则表达式`R_i`。
    
2. 把它们用 `|` 连接成一个巨大的正则表达式 `R = R_1 | R_2 | ... | R_k`。
    
3. 从输入字符串的开头开始，不断尝试匹配这个巨大的`R`，找到一个能匹配上的前缀。
    

#### **第14页：匹配过程中需要解决的问题**

这是自动识别中的核心难点，也是面试常考点。当一个字符串有好几种匹配方式时，听谁的？Lex工具给出了两条黄金法则：

1. **最长匹配原则 (Match the longest possible string)**：总是选择能匹配上的、最长的那段字符串。比如输入是`<=`，它既匹配模式`<`，也匹配模式`<=`。因为`<=`更长，所以词法分析器会识别出`<=`这一个词法单元，而不是`<`。
    
2. **优先匹配原则 (First rule priority)**：如果有多条规则都能匹配上**同样长度**的字符串，那么，在Lex规则文件里**写在最前面**的那条规则获胜。这通常用来区分关键字和标识符。比如，规则`if {return IF;}`会写在规则`{id} {return ID;}`的前面。这样，当输入是`if`时，虽然它既符合关键字`if`的规则，也符合标识符`id`的规则，但因为关键字规则在前，它会被优先识别为关键字。
    

#### **第15页：选择题**

这个问题就是检验你对上面两条黄金法则的理解。  
输入字符串是 "dictatorial"。  
规则有：(1) `dict`, (2) `dictator`, (3) `[a-z]*`, (4) `dictatorial`。

- **第一步：应用“最长匹配”原则。**
    
    - `dict`可以匹配规则(1)。
        
    - `dictator`可以匹配规则(2)。
        
    - `dictatorial`可以匹配规则(3)（任意字母串）和规则(4)。
        
    - 最长的匹配就是整个字符串 "dictatorial"。
        
- **第二步：应用“优先匹配”原则。**
    
    - 现在有两个规则 (3) 和 (4) 都匹配了最长的字符串 "dictatorial"。
        
    - 假设规则在文件中的顺序就是(1), (2), (3), (4)。
        
    - 规则(3)排在规则(4)的前面。
        
    - 因此，规则(3)胜出。
        

所以，最终"dictatorial"会被规则(3) `[a-z]*` 识别。正确答案是 **b)**。

#### **第17-24页：词法分析器生成工具—LEX**

这几页正式介绍了Lex这个工具。

- **第17页 (简介)**：重申了Lex的原理是把正则表达式翻译成DFA。
    
- **第18页 (过程)**：展示了使用Lex的流程：你写一个`.l`或`.lex`的源文件，Lex工具把它转换成一个`.c`文件（`lex.yy.c`），然后你用C编译器编译这个C文件，最终得到一个可执行的词法分析器。
    
- **第19-20页 (源程序结构)**：Lex源文件是三段式结构，用`%%`隔开：
    
    ```
    声明部分
    %%
    转换规则及动作
    %%
    辅助子程序
    ```
    
    核心是第二部分，它的格式是：`模式(正则表达式) { C语言动作 }`。
    
- **第21-23页 (工作方式和示例)**：通过一个非常完整的例子，展示了Lex代码怎么写。
    
    - `{ws} {/* no action and no return */}`：匹配任意空白符，动作为空，所以就是“吃掉并忽略”空白。
        
    - `if {return(IF);}`：匹配到字符串"if"，就返回一个叫`IF`的Token。
        
    - `{id} {yylval = ...; return(ID);}`：匹配到一个标识符，执行一些操作（比如存到符号表），然后返回一个叫`ID`的Token。
        
- **第24页 (冲突解决方法)**：再次总结了“最长匹配”和“优先匹配”这两条黄金法则。
    

#### **第26-27页：有限自动机 (Finite Automata)**

这里开始进入本节课的理论核心：有限自动机，它是实现词法分析的数学模型。

- **有限自动机** 是一个识别器，它只能回答“是”或“否”（即一个字符串是否符合某个模式）。
    
- 分为两类：
    
    - **确定的有限自动机 (DFA)**：路径唯一，无二义性。
        
    - **不确定的有限自动机 (NFA)**：路径不唯一，有多种可能。
        
- **第27页 (DFA定义)**：给出了DFA的严格数学定义，一个五元组 `\(Ｍ＝(Σ, Q, q0, F, δ)\)`。你不需要死记硬背，只需要理解每个部分的意思：
    
    - `\(Σ\)`: 字母表（允许出现的字符集合）。
        
    - `\(Q\)`: 状态的集合（图里所有的圈圈）。
        
    - `\(q0\)`: 开始状态（图里唯一的起始点）。
        
    - `\(F\)`: 接受状态的集合（图里所有的双圈圈）。
        
    - `\(δ\)`: 转换函数，即规则（`δ(当前状态, 输入字符) = 下一个状态`）。**“确定性”**就体现在这里：对于任何一个状态和输入，`δ`函数只会给**唯一**的一个“下一个状态”。
        

#### **第28-33页：DFA的表示和示例**

- **第28-29页**：DFA可以用“转换矩阵”（一个表格）或“状态转换图”来表示。图对人更直观，表格对计算机存储更方便。
    
- **第30-31页**：DFA接受的语言`L(M)`，就是所有能让这个DFA从开始状态走到任意一个接受状态的字符串的集合。
    
- **第32页**：给出了Pascal语言标识符和数字的DFA，这其实就是第8、9页状态转换图的另一种画法。
    
- **第33页 (DFA示例-2)**：这是一个非常经典的DFA，用于识别能被5整除的二进制数。
    
    - **原理**：这个DFA的5个状态（0, 1, 2, 3, 4）分别代表当前已读入的二进制数“除以5的余数”。
        
    - 比如你在余数是 `i` 的状态，如果新读入一个`0`，新数值 = `旧数值 * 2`，所以新余数 = `(i * 2) % 5`。
        
    - 如果新读入一个`1`，新数值 = `旧数值 * 2 + 1`，所以新余数 = `(i * 2 + 1) % 5`。
        
    - 你根据这个规则画箭头，就能得到这张图。开始时什么都没读，余数是0，所以在状态0。最终如果整个数读完后，机器停在状态0（余数为0），就说明这个数能被5整除。
        

#### **第35页：非确定的有限自动机 (NFA)**

NFA是DFA的“增强版”，它放宽了DFA的严格限制。

- **定义**：它的转换函数 `δ` 不再是返回一个唯一状态，而是返回一个**状态的集合**（`\(2^Q\)`）。
    
- 这意味着NFA有两大“超能力”：
    
    1. 在某个状态，收到一个输入字符后，可以**同时跳转到多个**后续状态。
        
    2. 可以有 `ε` (epsilon) 边，即**不读取任何字符**，就可以“免费”从一个状态跳转到另一个状态。
        
- 你可以把NFA想象成一个可以“分身”的机器人，遇到岔路口就分身成多个，每个分身走一条路。只要有**任何一个**分身最终走到了接受状态，就算成功。
    

#### **第36-38页：NFA示例和说明**

- **第36页**：这个NFA识别 `\((a|b)*aab\)`。你可以看到状态0对于`a`，既可以跳回状态0，也可以跳到状态1。这就是非确定性。
    
- **第37页**：这个NFA识别 `\(aa*|bb*\)`。从状态0开始，可以通过`ε`边，“免费”地选择进入上面的分支（准备识别`aa*`）还是下面的分支（准备识别`bb*`）。
    
- **第38页 (说明)**：这一页的结论至关重要：
    
    1. DFA是NFA的一种特殊情况（没有使用NFA超能力的NFA就是DFA）。
        
    2. **DFA和NFA的表达能力是等价的！** 这意味着，任何一个NFA能识别的语言，我们总能找到一个（可能更复杂的）DFA也能识别它。反之亦然。这个理论是整个词法分析自动生成工具（Lex）的基石，因为它保证了我们可以把一个容易构造的NFA（从正则表达式很容易得到NFA）转换成一个适合在计算机上高效执行的DFA。
        

#### **第39-40页：例题**

- **例题1**：给出 `\((a|b)*a(a|b)(a|b)\)` 的DFA。这个正则表达式描述的是“以a开头，后面跟任意两个a或b”的字符串。DFA需要记住最近的3个字符，才能判断是否满足条件。
    
- **例题2**：分析正则表达式 `\((1|01)*0*\)`。它描述的语言是“所有不包含子串`001`的`{0,1}`串”。给出的DFA有三个状态，含义分别是：
    
    - `start`：最近没有读到0。
        
    - `状态1`：最近连续读了1个0。
        
    - `状态2`：最近连续读了至少2个0。
        
    - 如果在状态2时读到一个1，就说明出现了`001`，此时应该进入一个“死亡状态”（图中未画出），即一个无论再读什么都无法回到接受状态的陷阱。只要不进入死亡状态，都是可接受的。
        

#### **第41页：作业**

这一页就是布置的作业了。

---

希望这份详细的讲解能帮助你补上这节课的知识。核心就是围绕着 **正则表达式 -> NFA -> DFA -> 代码** 这条主线，理解“最长匹配”和“优先匹配”两个原则，并能看懂简单的状态转换图。如果还有不清楚的地方，随时可以再问！

> 陷阱状态是怎么画的？

问得非常好！这是一个理解DFA（确定有限自动机）完整性的关键点。

**陷阱状态 (Trap State)**，也叫“死状态”(Dead State)，画起来很简单，它就是一个**普通的、非接受状态**，但它有一个非常特殊的性质：**所有从它出发的转换都会回到它自己**。

你可以把它想象成一个“黑洞”：一旦进去，就再也出不来了。

---

### 如何画陷阱状态

我们用上一题的 "不包含子串`001`" 的DFA来举例。

#### 1. 简化版图（省略陷阱状态，我们之前画的）

这是我们为了清晰而画的简化图。注意，在状态 `S2` 时，如果输入 `1`，是没有箭头的。在教学和交流中，我们默认这种情况就是进入了陷阱状态。

#### 2. 完整版图（明确画出陷阱状态）

要画出完整的DFA，我们需要补上这个陷阱状态（我们称它为 `S_trap`）。

**步骤：**

1. **画一个新状态**：画一个普通的圆圈（单圈，因为它是非接受状态），并标记为 `S_trap` 或 “陷阱”。
    
2. **连接“失败”转换**：从 `S2` 画一条箭头指向 `S_trap`，并标记为 `1`。这就表示，当看到 `00` 之后再看到 `1`，就“掉入陷阱”。
    
3. **画出“陷阱”的自循环**：从 `S_trap` 画一个指回自身的箭头，并标记上字母表中的**所有符号**（在这里是 `0, 1`）。这表示一旦进入陷阱，无论接下来输入什么，都只会停留在陷阱里，永远无法到达接受状态。
    

下面就是包含陷阱状态的完整DFA图：

---

### 总结陷阱状态的特点：

1. **外观**：它是一个**单圈**状态，表示非接受。
    
2. **功能**：它代表一个“不可逆的失败”。一旦DFA进入此状态，就意味着输入的字符串已经确定不符合语言的规则了。
    
3. **转换**：对于任何输入符号，它都**转换回自身**。
    
4. **作用**：它的存在使得DFA在数学上是“完备”的，即每个状态对于每个输入符号都**有且仅有**一个确定的转换。
    
5. **惯例**：在实际画图时，为了让图更简洁、突出主要逻辑，我们**常常省略**陷阱状态以及所有指向它的箭头。当你在一个DFA图中发现某个状态在某个输入上没有出路时，就应该明白它其实是去了一个被省略掉的陷阱状态。

# Lect 4 词法分析之三
好的，没问题。这节课的核心内容是编译器前端一个非常重要的环节：**词法分析**。

简单来说，这节课是为了教会你**如何将程序员写的、方便人阅读的“模式规则”（正则表达式），自动地转换成计算机可以高效执行的“状态机”（有限自动机），从而让计算机能够快速准确地识别出代码中的一个个“单词”（比如关键字 if、变量名 a、数字 100 等）**。

整个过程就像是把一张描述宝藏位置的草图（正则表达式），通过一系列标准化的步骤，绘制成一张精确的、一步一步跟着走的导航地图（确定的有限自动机 DFA），任何一个人（计算机程序）拿着这张地图都能找到宝藏。

接下来，我将按照课件的页码顺序，为你由浅入深地讲解其中的知识点。

---

### **第2-4页：回顾与目标**

这几页主要是回顾和设定目标。

- **回顾 (Review)**：我们在之前的课程里已经学习了两个核心概念：
    
    1. **正则表达式 (Regular Expression)**：一种用来描述字符串模式的简洁语言。比如，`(0|1)*0000` 可以表示所有以四个0结尾的二进制数，也即能被16整除的二进制数。这种方式非常适合人来书写和理解。
        
    2. **有限自动机 (Finite Automata)**：一个像流程图一样的模型，用来识别一个字符串是否符合某个模式。它有不同的状态，根据输入的字符在状态之间跳转。这种方式非常适合计算机来高效执行。
        
- **目标 (Goal)**：这节课的目标就是要把这两者连接起来。我们希望建立一个全自动的流程，把人写的“正则表达式”转换成机器可以高效执行的“DFA”（确定的有限自动机）。
    

### **第5页：从正则表达式到自动机的转换路径**

这一页的图非常关键，它揭示了整个转换过程的“路线图”：

正则表达式 (Regular Expression)→NFA (非确定的有限自动机)→DFA (确定的有限自动机)\text{正则表达式 (Regular Expression)} \rightarrow \text{NFA (非确定的有限自动机)} \rightarrow \text{DFA (确定的有限自动机)}

为什么不直接从正则表达式转到DFA呢？因为从正则表达式直接到DFA的算法很复杂，而通过NFA作为“中间商”则要简单和直观得多。

1. **第一步**：正则表达式 → NFA（Thompson构造法）
    
2. **第二步**：NFA → DFA（子集构造法）
    

接下来的课程就是围绕这两个核心步骤展开的。

### **第6-12页：NFA到DFA的转换（子集构造法）**

这是课程的第一个重点：如何将一个NFA转换成与之等价的DFA。

- **核心思想 (第6页)**：NFA和DFA的区别在于“不确定性”。NFA从一个状态接收一个输入后，可能跳转到多个状态，或者可以通过 `ε` (空字符串) 无条件地跳转。而DFA的每一步跳转都是唯一确定的。  
    “子集构造法”的核心思想就是：**DFA的一个状态，对应的是NFA的一组状态集合**。这样一来，NFA的不确定性就被DFA状态的“集合”属性给吸收了。
    
- **关键算法 (第9-10页)**：为了实现这个转换，我们需要两个关键的计算步骤：
    
    1. **`ε_closure(S)` (ε-闭包)**：计算从NFA的状态集合 `S` 出发，只通过 `ε` 边能够到达的所有状态的集合（包括 `S` 自身）。这相当于把所有“瞬移”能到的地方都算进来。
        
    2. **`DFAedge(d, a)` (DFA的转移函数)**：计算从DFA的当前状态 `d`（它是一个NFA状态集），接收一个字符 `a` 后，应该转移到的下一个DFA状态。计算方法是：  
        a. 找出 `d` 中所有状态接收 `a` 后能到达的NFA状态集合。  
        b. 再对这个新的NFA状态集合做一次 `ε_closure` 计算。
        
- **示例 (第11页)**：这一页通过一个具体的例子演示了转换过程。
    
    - **起始状态 A**: 是NFA起始状态 `0` 的 `ε_closure`，即 `ε_closure({0}) = {0, 1, 2, 4, 7}`。所以 DFA 的起始状态就是 A。
        
    - **从 A 出发**:
        
        - 输入 `a`：状态集合 `{0,1,2,4,7}` 中的状态 `2` 接收 `a` 到状态 `3`，状态 `7` 接收 `a` 到状态 `8`。所以我们得到集合 `{3, 8}`。对它求 `ε_closure`，得到 `{1,2,3,4,6,7,8}`，我们称之为状态 **B**。于是我们画一条从 A 到 B 的 `a` 边。
            
        - 输入 `b`：状态集合 `{0,1,2,4,7}` 中的状态 `4` 接收 `b` 到状态 `5`。对 `{5}` 求 `ε_closure`，得到 `{1,2,4,5,6,7}`，我们称之为状态 **C**。于是我们画一条从 A 到 C 的 `b` 边。
            
    - **重复这个过程**: 再分别从新的DFA状态 B 和 C 出发，计算它们接收 `a` 和 `b` 后的下一个状态，直到没有新的DFA状态产生为止。最终，我们就得到了第12页那个完整的DFA。
        

### **第13-19页：DFA的最小化**

通过子集构造法得到的DFA虽然是确定的，但可能有很多冗余的状态。为了让它更小、更高效，我们需要进行“最小化”。

- **核心思想 (第14页)**：找出所有“等价”的状态并将它们合并。什么是等价状态？如果从状态p和状态q出发，对于任意的输入字符串，最终要么都到达接受状态，要么都到达非接受状态，那么p和q就是等价的。简单说，它们的“未来”是完全一样的。
    
- **划分方法 (第17页)**：这是一个非常经典的分组算法。
    
    1. **初始划分**：先把所有状态粗略地分成两组：**接受状态组** 和 **非接受状态组**。因为这两组的状态行为肯定不等价。
        
    2. **迭代优化**：检查每一个组，看组内的状态是不是真的等价。方法是：任选一个输入字符（比如`a`），看组里的所有状态经过`a`转换后，是不是都跳到了同一个组里。如果有的状态跳到了A组，有的跳到了B组，那么这个组就“不纯”了，需要根据它们跳转的目标组，把当前这个组再分裂成更小的组。
        
    3. **终止条件**：不断重复第2步，检查所有输入字符，直到不管怎么检查，任何组都不再需要分裂为止。这时，同一个组里的所有状态就是等价的，可以合并成一个状态。
        
- **示例 (第18-19页)**：这里展示了最小化的过程。
    
    - 初始划分为 `{1,2,4,5,6,7}`（非接受状态）和 `{3}`（接受状态）。
        
    - 通过不断检查和分裂，最终将状态划分成了 `{1,2}`, `{4,5}`, `{6,7}`, `{3}` 这几个等价类，从而得到了一个更简洁的DFA。
        

### **第20-25页：从正则表达式到NFA的转换（Thompson构造法）**

现在我们回来看第一步：如何把正则表达式转换成NFA。Thompson构造法是一种非常优雅的递归方法。

- **核心思想 (第20-22页)**：它为每一种正则表达式的基本结构都定义了一个标准的“构建模块”，然后像搭积木一样把它们组合起来。
    
    - **基本情况 (第20页)**：
        
        - 对于 `ε`，构造一个开始状态和结束状态，中间用 `ε` 边连接。
            
        - 对于单个字符 `a`，构造一个开始状态和结束状态，中间用 `a` 边连接。
            
    - **递归组合 (第21-22页)**：
        
        - **并集 `r1|r2`**：新建一个开始状态和结束状态。从新的开始状态画两条 `ε` 边分别指向 `r1` 和 `r2` 的NFA的开始状态。再从 `r1` 和 `r2` 的NFA的结束状态画两条 `ε` 边指向新的结束状态。
            
        - **连接 `r1r2`**：直接把 `r1` 的NFA的结束状态和 `r2` 的NFA的开始状态合并。
            
        - **闭包 `r1*`**：新建一个开始状态和结束状态。从新的开始状态画 `ε` 边到 `r1` 的开始状态，从 `r1` 的结束状态画 `ε` 边到新的结束状态。同时，从 `r1` 的结束状态画 `ε` 边指回 `r1` 的开始状态（实现循环），再从新的开始状态画 `ε` 边直接到新的结束状态（实现匹配0次）。
            
- **示例 (第24-25页)**：这两页展示了如何用Thompson构造法为 `01*|1` 这个正则表达式一步步构建出对应的NFA。
    

### **第26-34页：词法分析器生成工具的设计（Lex/Flex）**

这部分是将前面的理论知识应用到实际工具中，比如著名的`Lex`工具。

- **工作原理 (第27-28页)**：`Lex`这类工具就是把我们上面讲的全自动地做了一遍。
    
    1. 你给它一系列的正则表达式（比如，关键字`if`的模式是`"if"`，标识符的模式是 `[a-zA-Z_][a-zA-Z0-9_]*`）。
        
    2. `Lex`内部对每个模式都用**Thompson构造法**生成一个NFA。
        
    3. 它将所有这些NFA用`ε`边连接到一个新的总的开始状态，**合并成一个巨大的NFA** (第29页)。这个大NFA可以识别你定义的所有模式。
        
    4. 然后，它用**子集构造法**将这个大NFA转换成一个DFA (第30页)。
        
    5. （可选）最后，它对这个DFA进行**最小化**。
        
    6. 最终，它生成C代码，代码里包含一个大大的状态转换表（代表这个最小化DFA）和一个驱动程序，这个程序就是最终的词法分析器。
        
- **冲突解决 (第30页)**：在合并NFA并转换为DFA后，可能会出现一个DFA的接受状态同时对应多个原始模式（比如输入`if`，既匹配关键字`if`的模式，也匹配标识符的模式）。`Lex`通常采用两条规则来解决冲突：
    
    1. **最长匹配原则**：总是选择能匹配输入字符串最长前缀的那个模式。
        
    2. **优先匹配第一规则**：如果多个模式匹配的长度相同，选择在定义文件中最先出现的那一个。
        
- **运行方式 (第34页)**：生成的词法分析器会不断读入字符，在DFA上模拟运行，直到走到一个“死胡同”（没有后续状态）。然后它会回溯到最后一个经过的“接受状态”，把从开始到这个接受状态的字符串作为一个词素（token）返回，并执行相应的动作。
    

### **第35-36页：总结与作业**

最后两页是对本章内容的总结，并布置了相应的练习，帮助你巩固。

### Q&A

> 我没太懂如何最小化一个DFA，可以详细解释一下如何操作吗？

当然可以。DFA最小化是编译原理中一个非常经典且重要的算法。我会为你详细拆解这个过程，力求让你彻底明白。

### **核心思想：合并“看不出区别”的状态**

想象一下，你站在DFA的某个状态 `p`，另一个人站在状态 `q`。现在，我随便给出一个输入字符串（比如 `1011`），你们俩都根据这个字符串在DFA上移动。如果**无论我给出什么字符串**，你们俩最终的结果都是一样的（要么都停在接受状态，要么都停在非接受状态），那么状态 `p` 和 `q` 就是“等价的”，或者叫“不可区分的”。

既然它们完全等价，我们就可以把它们合并成一个状态，从而减少状态总数，这就是DFA最小化的本质。

这个算法做的，就是通过一个系统性的方法，把所有状态划分成若干个组，**确保同一个组里的所有状态都是等价的**。

---

### **操作步骤：分组和分裂 (Partitioning and Refinement)**

下面我用一个比课件上更简单的例子来一步步演示，这样更容易理解。假设我们有以下DFA，输入符号集为 `{0, 1}`：

- **状态集合**：{A, B, C, D, E, F}
    
- **开始状态**：A
    
- **接受状态**：{F}
    
- **状态转移**：
    

|当前状态|输入 0|输入 1|
|:--|:--|:--|
|**A**|B|C|
|**B**|A|D|
|**C**|E|F|
|**D**|E|F|
|**E**|E|F|
|**F**|F|F|

#### **第1步：初始划分**

这是最基础的一步。一个接受状态和一个非接受状态肯定是**不等价**的（因为对于空字符串 `ε`，一个接受，一个不接受）。所以，我们先把所有状态分成两个大组：

- **G1**: 非接受状态组 {A, B, C, D, E}
    
- **G2**: 接受状态组 {F}
    

我们称这个初始划分为 **P₀ = { {A, B, C, D, E}, {F} }**

#### **第2步：迭代分裂（核心步骤）**

现在，我们要检查每个组，看组内的状态是不是真的“铁板一块”。检查的方法是：**看它们在接收同一个输入后，是否会“走向不同的未来（即跳到不同的组里）”**。

**检查 P₀ 中的 {A, B, C, D, E} 组：**

1. **对于输入 `0`**：
    
    - A on 0 → B (B在G1里)
        
    - B on 0 → A (A在G1里)
        
    - C on 0 → E (E在G1里)
        
    - D on 0 → E (E在G1里)
        
    - E on 0 → E (E在G1里)
        
    - **结论**：对于输入`0`，组内所有状态的下一个状态都在同一个组（G1）里。暂时看不出区别。
        
2. **对于输入 `1`**：
    
    - A on 1 → C (C在G1里)
        
    - B on 1 → D (D在G1里)
        
    - C on 1 → F (F在**G2**里) <-- **出现不同！**
        
    - D on 1 → F (F在**G2**里) <-- **出现不同！**
        
    - E on 1 → F (F在**G2**里) <-- **出现不同！**
        
    - **结论**：我们发现，对于输入 `1`，状态 A 和 B 的下一步去了 G1 组，而状态 C, D, E 的下一步去了 G2 组。这说明 A, B 和 C, D, E 是**不等价**的！因此，我们必须根据这个差异把 {A, B, C, D, E} 这个组**分裂**。
        

分裂后的新划分为：

- **G1'**: {A, B} (因为它们输入1后都去了G1)
    
- **G2'**: {C, D, E} (因为它们输入1后都去了G2)
    
- **G3'**: {F} (原来的G2，只有一个状态，不用动)
    

我们得到了新的划分 **P₁ = { {A, B}, {C, D, E}, {F} }**

#### **第3步：重复迭代，直到稳定**

现在我们有了新的划分P₁，我们要重复第2步，检查P₁中的每一个组。

**检查 P₁ 中的 {A, B} 组：**

- 对于输入 `0`：A→B, B→A。它们的下一个状态都在 {A, B} 组内。OK。
    
- 对于输入 `1`：A→C, B→D。它们的下一个状态都在 {C, D, E} 组内。OK。
    
- **结论**：{A, B} 组很稳定，不需要再分裂。
    

**检查 P₁ 中的 {C, D, E} 组：**

- 对于输入 `0`：C→E, D→E, E→E。它们的下一个状态都在 {C, D, E} 组内。OK。
    
- 对于输入 `1`：C→F, D→F, E→F。它们的下一个状态都在 {F} 组内。OK。
    
- **结论**：{C, D, E} 组也很稳定，不需要再分裂。
    

**检查 P₁ 中的 {F} 组：**

- 只有一个状态，不可能分裂。
    

我们对新的划分 P₁ 进行了一整轮检查，发现没有任何组需要再分裂了。这意味着算法已经**收敛**，划分已经稳定。

最终的等价类划分就是 **P_final = { {A, B}, {C, D, E}, {F} }**

#### **第4步：构建最小化DFA**

现在我们有了最终的等价类，可以构建新的DFA了：

1. **新状态**：每一个等价类组就是一个新的状态。
    
    - S₀ = {A, B}
        
    - S₁ = {C, D, E}
        
    - S₂ = {F}
        
2. **新开始状态**：包含原始开始状态（A）的那个组。
    
    - 新的开始状态是 S₀。
        
3. **新接受状态**：包含原始接受状态（F）的那些组。
    
    - 新的接受状态是 S₂。
        
4. **新转移函数**：要确定 S₀ 在输入 `0` 时去哪里，我们随便从 S₀ 组里选一个状态（比如A），看它原来去哪。A on 0 → B。B 属于哪个新状态？B 属于 S₀。所以，**S₀ on 0 → S₀**。
    
    - S₀ on 1: 选 A，A on 1 → C。C 属于 S₁。所以 **S₀ on 1 → S₁**。
        
    - S₁ on 0: 选 C，C on 0 → E。E 属于 S₁。所以 **S₁ on 0 → S₁**。
        
    - S₁ on 1: 选 C，C on 1 → F。F 属于 S₂。所以 **S₁ on 1 → S₂**。
        
    - S₂ on 0: 选 F，F on 0 → F。F 属于 S₂。所以 **S₂ on 0 → S₂**。
        
    - S₂ on 1: 选 F，F on 1 → F。F 属于 S₂。所以 **S₂ on 1 → S₂**。
        

最终，我们得到了一个只有3个状态的最小化DFA，它和原来6个状态的DFA能识别完全相同的语言。

这就是DFA最小化的完整操作流程。你课件第18页的例子也是完全按照这个“初始划分 -> 迭代分裂 -> 构建新DFA”的流程来做的，只是状态更多，计算起来稍微复杂一点而已。希望这个详细的步骤能帮助你理解！

# Lect 5 语法分析之一

好的，没问题。这门《编译原理》的核心课程确实需要花些心思理解。我会一页一页地为你讲解这份课件，力求清晰易懂，帮你把缺的课补上。

我们开始吧。

---

### **第 1-2 页: 课程简介与内容提要**

- **第 1 页** 是课程的标题页，告诉你这是《编译原理》课程的第四章，主题是“语法分析（Syntax Analysis）”的第一部分。
    
- **第 2 页** 是本章内容的提纲。你可以把它看作是学习路线图。本章会介绍：
    
    - **语法分析简介**：语法分析是干什么的？
        
    - **上下文无关文法 (Context-Free Grammar, CFG)**：这是描述编程语言语法的核心工具。
        
    - **文法的设计方法**：如何写出好的文法规则。
        
    - **自顶向下/自底向上分析**：两种主流的语法分析方法。
        
    - **LR 分析**：一种强大的自底向上分析技术。
        
    - **二义性文法**：一种可能会引起问题的文法。
        
    - **YACC**：一个能自动生成语法分析器的工具。
        

这节课会重点讲解前面几个基础概念，为后面的具体分析方法打下基础。

---

### **第 3 页: 为什么要用文法描述语言**

这一页解释了我们为什么需要一个叫做“上下文无关文法 (CFG)”或“BNF”的东西来描述编程语言。

简单来说，编程语言和我们日常说的汉语、英语一样，都有自己的“语法规则”。比如，C 语言里 `if` 语句必须是 `if (条件) { ... }` 这种格式。我们不能随便写，编译器需要一个精确、无歧义的方式来理解和检查这些规则。

文法就是这个精确的工具，它有几个好处：

1. **精确易懂**：用数学化的方式定义语法，没有模糊不清的地方。
    
2. **自动化**：有了文法，我们就可以用 YACC 这样的工具自动生成语法分析器，不用自己从头手写，效率很高。
    
3. **结构清晰**：文法揭示了代码的层次结构（比如一个函数包含多条语句，一条语句又包含表达式），这对于后续的语义分析和代码生成非常重要。
    
4. **易于维护**：如果想给语言增加新功能（比如新的循环语句），只需要修改或增加几条文法规则就行了。
    

---

### **第 4 页: 语法分析器的作用**

这张图非常关键，它展示了语法分析器在编译器中的位置和作用。

1. **输入**：语法分析器的输入是**词法分析器**（上一章学的）处理后的结果，也就是一串 **Token**（单词符号）。比如，对于代码 `a = b + c;`，词法分析器会输出类似 `[标识符, a]`, `[赋值号, =]`, `[标识符, b]`, `[加号, +]`, `[标识符, c]`, `[分号, ;]` 这样的 Token 序列。
    
2. **工作**：语法分析器的核心任务就是检查这串 Token 是否符合语言的语法规则。它就像一个语法老师，判断你写的句子“主谓宾”是否正确。
    
3. **输出**：如果语法正确，它会构建一棵“**分析树**”（Parse Tree）。这棵树清晰地展示了代码的结构。比如，它会告诉你 `b + c` 是一个加法表达式，而整个 `a = b + c;` 是一个赋值语句。如果语法有错误（比如写了 `a = b + ;`），它就会报错。
    

**核心任务**：判断一串输入的 Token 能否构成一个语法上合法的程序单元（比如一个合法的表达式、一条合法的语句）。

---

### **第 5 页: 语法分析器的种类**

语法分析器主要分为三类，但实际应用中主要是后两种：

1. **通用语法分析器**：理论上能分析任何类型的文法，非常强大，但效率极低，不适合用在需要高性能的编译器中。
    
2. **自顶向下的语法分析器 (Top-Down Parser)**：从“目标”出发，尝试推导出我们实际输入的 Token 串。比如，目标是分析一个“语句”，它会去看语法规则，“语句”可以是一个“赋值语句”，然后看“赋值语句”的构成，一步步向下分解，直到匹配上输入的 Token。
    
3. **自底向上的语法分析器 (Bottom-Up Parser)**：与上面相反，它从输入的 Token 出发，尝试将它们一步步组合、归纳成更大的语法单元，最终目标是组合成最顶层的“程序”。比如，它看到 `b`, `+`, `c` 三个 Token，根据语法规则把它们组合成一个“表达式”，然后再把这个“表达式”和 `a`, `=` 组合成一个“赋值语句”。
    

**共同点**：无论是自顶向下还是自底向上，它们通常都是**从左到右**扫描输入的 Token。为了效率，它们只适用于特定类型的文法，但这些文法已经足够强大，可以描述我们常见的大部分编程语言了。

---

### **第 6-8 页: 用自然语言的例子理解文法**

这几页用一个英语句子的例子来帮助我们直观地理解什么是文法。

- **第 6 页**：展示了一句 "The grey wolf will eat the goat" 的分析树。它把整个句子（`<句子>`）分解为主语和谓语，主语又分解为冠词、形容词、名词等，结构非常清晰。
    
- **第 7 页**：把这种分解规则用形式化的“产生式”写了出来。
    
    - `<句子> → <主语> <谓语>` 这条规则读作：“一个句子由一个主语后跟一个谓语构成”。
        
    - 这里的尖括号 `<...>` 里的内容，如 `<句子>`、`<主语>`，是抽象的语法概念，我们称之为**非终结符号 (Nonterminal)**。
        
    - 而像 "the", "grey", "wolf" 这样实际的单词，是构成句子的最小单位，我们称之为**终结符号 (Terminal)**。
        
- **第 8 页**：正式定义了文法的四个组成部分，这是一个非常重要的定义 G = (VT, VN, S, P)。
    
    - **VT (Terminal Set)**：终结符号的集合。就是语言的基本词汇，比如例子中的 `{the, grey, wolf, ...}`。在编程语言里，就是 `if`, `+`, `*`, 标识符等 Token。
        
    - **VN (Nonterminal Set)**：非终结符号的集合。就是抽象的语法单元，比如例子中的 `{<句子>, <主语>, ...}`。
        
    - **S (Start Symbol)**：开始符号。是最高级的语法单元，我们分析的目标就是从这个符号开始，比如例子中的 `<句子>`。
        
    - **P (Production Rules)**：产生规则的集合。就是第 7 页列出的所有 `→` 规则，它定义了非终结符号如何被替换和展开。
        

---

### **第 9-10 页: 推导 与 语法/语义之分**

- **第 9 页**：展示了**推导 (Derivation)** 的过程。推导就是从开始符号 `S` 出发，不断地应用产生式规则，用规则的右边替换左边的非终结符号，一步步地最终得到一个只包含终结符号的句子。这个过程就是用文法“生成”一个合法句子的过程。语法分析做的其实是反向工作——判断一个给定的句子是否能通过推导得出。
    
- **第 10 页**：强调了**语法 (Syntax)** 和 **语义 (Semantics)** 的区别。
    
    - **语法**只关心句子的结构是否正确。比如 "the grey goat will eat the wolf" 这句话，从语法结构上看是完全正确的（主谓宾结构完整）。语法分析器会认为它是合法的。
        
    - **语义**则关心句子的含义是否合理。在现实世界中，山羊 (goat) 不会吃狼 (wolf)。这种意义上的正确性检查，不归语法分析管，而是由后续的**语义分析**阶段来处理。
        

---

### **第 11-14 页: 文法的正式定义与表示法**

- **第 11 页**：给出了文法的一个更通用的数学定义。这里我们主要关注 `2 型文法`。同时提到了产生式的简写形式，如果一个非终结符 `A` 可以推导出多个不同的形式，比如 `A → α1` 和 `A → α2`，可以简写为 `A → α1 | α2`，这里的 `|` 读作“或”。
    
- **第 12 页**：定义了**上下文无关文法 (Context-Free Grammar, CFG)**。这是本章的绝对核心。它的特点是：**所有产生式的左边都只有一个非终结符号**，形式为 A→β。
    
    - “上下文无关”的意思是，无论 `A` 出现在什么环境（上下文）中，我们都可以用 `β` 来替换它。这大大简化了分析过程，也是为什么它在编程语言中被广泛应用的原因。
        
- **第 13-14 页**：介绍了 **BNF (Backus-Naur Form)**。BNF 本质上就是上下文无关文法的另一种写法，它用 `::=` 来代替 `→`。在很多编程语言的官方文档中，你都会看到用 BNF 来描述语法。第 14 页是一个有趣的例子，它用 BNF 语法来定义 BNF 语法本身，展示了其表达能力。
    

---

### **第 15-19 页: 文法实例与推导过程**

- **第 15 页**：给出了一个非常经典的算术表达式文法 G1[E]。这个文法用来描述像 `a+a*a` 这样的表达式。
    
    - `E` 代表表达式 (Expression)，`T` 代表项 (Term)，`F` 代表因子 (Factor)。
        
    - 这个文法通过层次设计（表达式由项构成，项由因子构成）巧妙地实现了运算符的**优先级**（乘法 `*` 优先于加法 `+`）和**结合性**。
        
- **第 16 页**：介绍了一些书写文法时的**约定**，目的是让文法更容易读懂。比如，通常用大写字母表示非终结符，小写字母表示终结符；第一个产生式的左部默认为开始符号等。
    
- **第 17-19 页**：详细定义了**推导 (Derivation)** 和 **归约 (Reduction)**。
    
    - **直接推导 (⇒)**：应用一次产生式规则。
        
    - **推导 (⇒* 或 ⇒+)**：经过零次或多次 (⇒*) 或一次或多次 (⇒+) 推导。如果能从开始符号 S 推导出某个串 w（记作 S⇒∗w），并且 w 只包含终结符，那么 w 就是这个文法所定义的语言中的一个句子。
        
    - **归约**是推导的逆过程，是自底向上分析的核心操作。
        

---

### **第 20 页: 最左推导和最右推导**

当一个句型中含有多个非终结符时，我们下一步该替换哪一个呢？这就引出了两种约定的推导方式：

- **最左推导 (Leftmost Derivation)**：在每一步推导中，总是选择**最左边**的那个非终结符进行替换。
    
- **最右推导 (Rightmost Derivation)**：在每一步推导中，总是选择**最右边**的那个非终结符进行替换。
    

对于一个无二义性的文法，虽然推导的步骤不同，但这两种推导方式最终会得到相同的分析树。

---

### **第 21-24 页: 句型、句子、语言和分析树**

- **第 21 页**：给出了三个重要概念的正式定义：
    
    - **句型 (Sentential Form)**：从开始符号 S 推导出来的任何字符串（可以包含非终结符和终结符）。是推导过程中的中间产物。
        
    - **句子 (Sentence)**：一个只包含终结符的句型。是推导的最终结果，是语言里的一个合法字符串。
        
    - **语言 L(G)**：由文法 G 生成的**所有句子**的集合。
        
- **第 22-24 页**：进一步解释了**分析树 (Parse Tree)**。
    
    - 分析树是推导过程的图形化表示，非常直观。
        
    - 树的根是开始符号，内部节点是非终结符，叶子节点是终结符。
        
    - 一棵分析树唯一地确定了句子的语法结构。
        
    - 第 23 和 24 页分别展示了如何从自顶向下（推导）和自底向上（归约）的角度来构建同一棵分析树。
        

---

### **第 25-29 页: 文法的二义性 (Ambiguity)**

这是一个非常重要的概念。

- **第 25-27 页**：什么是**二义性**？如果一个文法对于某个句子，可以产生**两棵或更多不同**的分析树，那么这个文法就是**二义性文法**。
    
    - 例子中给了一个简单的表达式文法 `E → E+E | E*E | ...`。对于句子 `a+a*a`，它可以有两种不同的分析方式（两种不同的分析树），一种对应 `(a+a)*a`，另一种对应 `a+(a*a)`。
        
    - 编译器无法确定哪种是正确的，这会导致生成的代码行为不确定，这是绝对不能接受的。
        
- **第 28-29 页**：对二义性的说明。
    
    - **定义**：一个文法是二义性的，如果它能为某个句子生成多棵分析树。
        
    - **如何处理**：
        
        1. **改写文法**：通常我们可以通过改写文法来消除二义性。比如第 15 页的文法 G1[E] 就是无二义性的，它通过引入 `T` 和 `F` 强制规定了乘法优先于加法。
            
        2. **保留二义性文法，附加规则**：有时二义性文法更简洁直观。比如 `if-then-else` 语句的文法就存在经典的二义性（`else` 不知与哪个 `if` 匹配）。此时我们可以在语法分析器中附加一个规则，比如“`else` 总是与最近的未匹配的 `if` 结合”，来解决这个问题。
            

---

### **第 30-34 页: 更多关于文法的理论**

这部分内容偏理论，帮助你更深入地理解文法。

- **第 30 页**：指出了两个理论上的要点：1. 判定一个任意的 CFG 是否是二义性的是一个“不可判定问题”，即没有一个通用算法能解决。2. 存在一些语言，它们天生就是二义性的（**先天二义性语言**），无论怎么设计文法，都无法避免二义性。
    
- **第 31-34 页**：介绍了如何用**数学归纳法**来证明一个文法 G 生成的语言 L(G) 正是我们想要的语言 L。
    
    - 证明分为两步：
        
        1. 证明 L(G) ⊆ L：即证明 G 生成的每个句子都属于 L。
            
        2. 证明 L ⊆ L(G)：即证明 L 中的每个字符串都能由 G 生成。
            
    - 32-34 页用一个例子（所有对称括号对的串）完整地展示了这个证明过程。
        

---

### **第 35-37 页: 上下文无关文法 vs 正则表达式**

这里比较了两种描述语言的工具的能力。

- **结论**：上下文无关文法 (CFG) 的表达能力**比**正则表达式**更强**。
    
- **第 35 页**：证明了这一点。
    
    - 存在一些语言，比如 L = {aⁿbⁿ | n>0}（n 个 a 后面跟 n 个 b），可以用 CFG 描述（`S → aSb | ab`），但无法用正则表达式描述。原因是正则表达式（或有限自动机）没有“记忆力”，它无法记住前面有多少个 a，以便在后面匹配同样数量的 b。
        
- **第 36-37 页**：证明了所有正则语言都可以用 CFG 来描述。也就是说，正则表达式能干的活，CFG 都能干。它还给出了一个从 NFA（非确定有限自动机）构造等价 CFG 的算法。
    

---

### **第 38-41 页: 文法能力的局限与乔姆斯基谱系**

- **第 38-39 页**：指出了连 CFG 也**无法描述**的一些语言结构。
    
    - 例如，检查变量是否“先声明后使用”（对应 `wcw` 模型），或者函数调用时实参和形参的个数是否一致（对应 `aⁿbᵐcⁿdᵐ` 模型）。这些都需要“上下文”信息，而 CFG 是“上下文无关”的。
        
    - 在实际的编译器中，这些检查不是由语法分析器完成的，而是交给了后续的**语义分析**阶段。
        
- **第 40 页**：介绍了著名的**乔姆斯基谱系 (Chomsky Hierarchy)**，它根据产生式规则的限制，将文法分为了四种类型：
    
    - **0 型（无限制文法）**：能力最强，对应图灵机。
        
    - **1 型（上下文有关文法）**：规则形式为 `αAβ → αγβ`，替换 `A` 时需要考虑上下文 `α` 和 `β`。
        
    - **2 型（上下文无关文法）**：我们这章的重点，规则形式为 `A → β`。
        
    - **3 型（正则文法）**：限制最严格，能力最弱，对应正则表达式和有限自动机。
        
- **第 41 页**：总结了这些文法在编译器中的实际应用。
    
    - **词法分析**用 **3 型文法**（正则文法）。
        
    - **语法分析**用 **2 型文法**（上下文无关文法）。
        
    - 更复杂的**上下文相关**的检查（如类型匹配、参数匹配）通过**语义分析**来完成。
        
    - 这体现了编译器设计中“关注点分离”的思想，用合适的工具解决合适的问题。
        

---

### **第 42 页: 作业**

这一页是作业布置，提醒你在 10 月 10 日前完成练习。

---

希望这份详细的讲解能帮助你理解这节课的内容。编译原理中，语法分析是非常承上启下的一章，理解好文法、推导、分析树、二义性这些基本概念，对后续学习具体的分析算法至关重要。如果还有不清楚的地方，随时可以再问！



# Lect 6 语法分析之二

好的，没问题。这节课的核心内容是**自顶向下（Top-Down）的语法分析方法**。为了让这种方法高效可行，我们需要对“文法”（Grammar）本身进行一些改造，这门课件详细讲解了如何改造文法（消除二义性、消除左递归、提取左公因子），以及如何基于改造后的文法构建一个不需要“回溯”的、高效的语法分析器（预测分析器）。

下面我将按照课件的页码顺序，为你由浅入深地讲解每一页的内容。

---

### **第 1-4 页：回顾与练习**

#### **第 1-2 页：标题与回顾**

- **内容简介**：这两页是课程标题和对上一节课内容的回顾。核心概念是**上下文无关文法 (Context-Free Grammar, CFG)**。
    
- **阐述**：
    
    - 文法就像是语言的“骨架”，它定义了什么样的符号串是合法的。例如，文法 `E→E+E | E*E | (E) | a` 定义了一种简单的算术表达式。
        
    - **推导 (Derivation)** 是指从开始符号（这里是 `E`）出发，根据文法规则一步步替换非终结符（大写字母），最终得到一个只包含终结符（如 `+`, `*`, `a`）的符号串的过程。这个过程模拟了编译器构建语法结构的方式。幻灯片中的例子 `E→E+E →a+E ... →a+a*a` 展示了一个推导过程。
        

#### **第 3 页：练习-1 (二义性问题)**

- **内容简介**：这页通过一个练习引出了**文法的二义性 (Ambiguity)** 问题。
    
- **阐述**：
    
    - 问题：对于文法 `E→E*E | E+E | (E) | int`，字符串 `5*3+(2*7)+4` 有多少种不同的分析树？
        
    - 这里的核心问题是：给定的文法没有定义运算符的**优先级**（`*` 优先于 `+`）和**结合性**（`+` 是从左到右计算）。
        
    - 例如，对于 `5*3+2`，文法允许两种解释：`(5*3)+2` 和 `5*(3+2)`。每种解释对应一棵不同的分析树。
        
    - 这页给出的5个答案，实际上是5种不同的结合方式，代表了5棵不同的分析树。这说明原始文法是**二义性**的。一个文法如果对某个句子能产生多于一棵的分析树，它就是二义性的。二义性文法在编程中是不允许的，因为计算机会不知道该按哪种方式去执行代码。
        

#### **第 4 页：练习-2 (文法生成的语言与分析树)**

- **内容简介**：这个练习让你理解一个文法能生成的所有句子（称为该文法的语言），并再次强调二义性（分析树数量 > 句子数量）。
    
- **阐述**：
    
    - 通过分析给出的文法规则，我们可以推导出所有可能的句子：
        
        - `1`: 来自 `S → A1 → ε1` 或 `S → 1B → 1ε`。
            
        - `11`: 来自 `S → A1 → C1 → 11`。
            
        - `111`: 来自 `S → 1B → 1C1 → 111`。
            
        - `01`: 来自 `S → A1 → C1 → 01`。
            
        - `101`: 来自 `S → A1 → 101` 或 `S → 1B → 1C1 → 101`。
            
    - 我们一共能生成5个不同的句子。
        
    - 为什么有7棵分析树呢？因为其中一些句子不止一种推导方式。
        
        - 句子 `1` 有两种生成方式（`S→A1...` 和 `S→1B...`），所以有2棵分析树。
            
        - 句子 `101` 也有两种生成方式（`S→A1...` 和 `S→1B...`），所以有2棵分析树。
            
        - 其他三个句子 `11`, `111`, `01` 各只有一种生成方式。
            
    - 总计：2 + 2 + 1 + 1 + 1 = 7 棵分析树。这再次说明，这个文法是二义性的。
        

---

### **第 5-19 页：文法的设计方法**

这部分是本节课的第一个重点：如何“修理”有问题的文法，让它变得好用。

#### **第 6 页：文法的设计方法概述**

- **内容简介**：介绍了文法在程序设计语言中的作用和局限性，并引出了三种关键的文法变换方法。
    
- **阐述**：
    
    - 文法能描述大部分语法规则（比如 `if` 语句的结构），但无法描述所有规则（比如变量必须先声明后使用，这属于**语义**范畴）。因此，语法分析器之后还需要语义分析器来做更深入的检查。
        
    - 为了让语法分析器能顺利工作，我们常常需要对文法进行三种改造：
        
        1. **消除二义性**：确保每个句子只有一种解释。
            
        2. **消除左递归**：避免分析器陷入死循环。
            
        3. **提取左公因子**：帮助分析器做出唯一的选择。
            

#### **第 7-9 页：消除文法的二义性**

- **内容简介**：通过两个经典例子讲解如何消除二义性。
    
- **阐述**：
    
    - **第 7-8 页 (Dangling-else 问题)**：这是最经典的二义性例子。对于 `if E1 then if E2 then S1 else S2`，`else` 到底跟哪个 `if` 配对？
        
        - **问题**：文法允许 `else` 跟第一个 `if` 配对，也允许它跟第二个 `if` 配对，产生二义性。
            
        - **解决方法**：编程语言的通用规则是“`else` 与最近的、未配对的 `then` 匹配”。为了在文法中强制实现这一点，我们引入了两个新的非终结符 `matched_stmt` (已匹配的语句，即 `if-then-else` 结构) 和 `open_stmt` (开放的语句，即只有 `if-then` 的结构)。通过规则限制 `then` 后面只能跟 `matched_stmt`，就强制 `else` 必须先匹配内部的 `if`。
            
        - **结论**：消除二义性没有固定的公式，需要具体问题具体分析。
            
    - **第 9 页 (另一个例子)**：文法 `S → aSb | bSa | SS | ...` 也能产生二义性。例如 `ababab` 可以通过多种方式推导。课件直接给出了一个等价的、无二义性的文法，这再次说明消除二义性是一种技巧，有时甚至相当复杂。
        

#### **第 10-16 页：消除文法中的左递归**

- **内容简介**：讲解什么是左递归以及如何消除它，这是自顶向下分析的**必要准备工作**。
    
- **阐述**：
    
    - **什么是左递归？** 如果一个非终结符 `A` 可以推导出以它自身开头的字符串（即 `A ⇒⁺ A...`），就称文法含有左递归。
        
    - **为什么是问题？** 对于自顶向下的分析器，当它试图用 `A` 的规则去匹配输入时，如果规则是 `A → Aα`，它会立即再次调用处理 `A` 的过程，但输入指针根本没有移动，从而导致**无限循环**。
        
    - **第 10 页 (消除直接左递归)**：
        
        - **直接左递归**形如 `A → Aα | β`（其中 `β` 不以 `A` 开头）。
            
        - **消除方法**：把它变成等价的右递归形式。想象一下 `A` 生成的串是 `β` 后面跟着零个或多个 `α`。我们可以这样描述：
            
            - `A → βA'` (一个 `A` 首先是个 `β`，后面跟着“尾巴” `A'`)
                
            - `A' → αA' | ε` (“尾巴” `A'` 要么是另一个 `α` 加上更长的尾巴，要么是空 `ε`)
                
    - **第 11 页 (例子)**：将算术表达式文法 `E → E + T | T` 中的左递归消除。`E` 就是 `A`，`+T` 就是 `α`，`T` 就是 `β`。套用公式即可得到 `E → TE'` 和 `E' → +TE' | ε`。
        
    - **第 12 页 (消除间接左递归)**：
        
        - **间接左递归**是指推导经过多步才回到自身，例如 `S → Aa`, `A → Sd`，这里 `S ⇒ Aa ⇒ Sda`。
            
        - **消除方法**：通过**代入**法，将间接左递归转变为直接左递归，然后再用上面的方法消除。
            
    - **第 13-16 页 (通用算法和示例)**：给出了一个消除所有左递归的通用算法。核心思想是：
        
        1. 给所有非终结符排个序 (如 `A₁, A₂, ..., Aₙ`)。
            
        2. 依次处理每个 `Aᵢ`，在处理 `Aᵢ` 时，把它规则中所有以 `Aⱼ (j < i)` 开头的产生式，用 `Aⱼ` 的规则展开代入。
            
        3. 这样处理完后，`Aᵢ` 的规则中如果存在左递归，必然是直接左递归，再用标准方法消除。
            
        4. 第14-16页完整地演示了这个复杂的过程，展示了不同排序会导致形式不同但等价的文法。
            

#### **第 17-19 页：提取左公因子**

- **内容简介**：讲解另一种文法改造——提取左公因子，同样是为了帮助分析器做出唯一选择。
    
- **阐述**：
    
    - **问题**：如果一个非终结符 `A` 的多个产生式有共同的前缀，如 `A → αβ₁ | αβ₂`，那么分析器在看到输入 `α` 时，不知道该选择 `αβ₁` 还是 `αβ₂` 这条路。
        
    - **解决方法 (提取左公因子)**：把公共部分 `α` 提取出来，剩下的部分用一个新的非终结符 `A'` 表示。
        
        - `A → αA'`
            
        - `A' → β₁ | β₂`
            
    - 这样，分析器看到 `α` 时，可以唯一地选择 `A → αA'`，然后再根据 `α` 后面的输入来决定 `A'` 的推导。
        
    - **第 19 页 (例子)**：再次用 `if-then-else` 文法举例，`if expr then stmt ...` 是公共前缀，提取后得到更适合预测分析的文法。
        

---

### **第 20-48 页：自顶向下的语法分析**

这部分是课程的核心，讲解了自顶向下分析的工作原理，以及如何构建一个高效的、无需回溯的预测分析器。

#### **第 20-26 页：自顶向下分析与回溯问题**

- **内容简介**：介绍了自顶向下分析的基本思想，并指出了“回溯”带来的低效率问题。
    
- **阐述**：
    
    - **自顶向下分析**：从文法的开始符号出发（树的根），尝试推导出与输入字符串完全匹配的分析树。这个过程就像在走一个迷宫，每次遇到非终结符，就要选择一个产生式往下走。
        
    - **预测/试探**：如果一个非终结符有多个产生式，分析器需要“猜”一个。如果猜错了（后续无法匹配输入），就必须**回溯**（退回到上一步），然后尝试另一个选择。
        
    - **回溯的缺点**：效率极低。最坏情况下，时间复杂度是指数级的。
        
    - **如何避免回溯？** 我们希望分析器能够“预知未来”，在每个选择点，通过向后看一个或几个输入符号（称为**向前看 lookahead**），就能**唯一确定**要使用哪个产生式。这就是**预测分析 (Predictive Parsing)** 的目标。
        

#### **第 27-35 页：First集、Follow集与LL(1)文法**

- **内容简介**：为了实现预测分析，我们引入了两个非常重要的数学工具：**First 集**和 **Follow 集**。它们是构建预测分析器的理论基础。
    
- **阐述**：
    
    - **第 27 页 (定义)**：
        
        - **`First(α)`**：`α` 是一个文法符号串。`First(α)` 指的是从 `α` 推导出的所有字符串的**第一个终结符**的集合。如果 `α` 能推导出空串 `ε`，那么 `ε` 也在 `First(α)` 中。
            
            - _通俗理解_：`First(A)` 就是 `A` 这个语法结构所有可能的“开头长相”。
                
        - **`Follow(A)`**：`A` 是一个非终结符。`Follow(A)` 指的是在所有合法的句型中，**紧跟在 `A` 后面的终结符**的集合。如果 `A` 可能出现在句子的末尾，那么句子结束符 `$` 就在 `Follow(A)` 中。
            
            - _通俗理解_：`Follow(A)` 就是 `A` 这个语法结构后面可能遇到的“邻居”。
                
    - **第 28-31 页 (计算方法和例子)**：详细讲解了如何计算 First 集和 Follow 集。这套算法是固定的，需要仔细理解和练习。
        
    - **第 32 页 (LL(1)文法)**：这是本节课最重要的定义之一。一个文法被称为 **LL(1)** 文法，当且仅当对于任何一个非终结符 `A` 的任意两个不同产生式 `A → α` 和 `A → β`，满足以下条件：
        
        1. `First(α)` 和 `First(β)` 没有交集。（保证不同产生式的“开头长相”不同）
            
        2. 如果其中一个（比如 `β`）可以推导出空串 `ε`，那么 `First(α)` 和 `Follow(A)` 没有交集。（保证当 `A` 可以“消失”时，它的“开头长相”和它的“邻居”长相不冲突）。
            
        
        - **LL(1) 的意义**：`L` (Left-to-right scan) 从左到右扫描输入，第二个 `L` (Leftmost derivation) 生成最左推导，`(1)` (1 symbol lookahead) 只需要向前看1个符号。LL(1)文法就是可以通过向前看1个符号来唯一确定下一步推导的文法，它**保证了预测分析不会产生回溯**。我们之前做的消除左递归、提取左公因子等工作，很大程度上就是为了把一个文法改造成 LL(1) 文法。
            
    - **第 34-35 页 (预测分析表)**：
        
        - 有了 First 和 Follow 集，我们就可以构建一张**预测分析表 M[A, a]**。这张表就是预测分析器的“大脑”。
            
        - 表 M 的行是非终结符 `A`，列是终结符 `a`（包括 `$`）。
            
        - `M[A, a]` 的内容告诉分析器：当栈顶符号是 `A`，下一个输入符号是 `a` 时，应该使用哪个产生式。
            
        - **构造规则**在第 34 页有清晰的描述，它完全基于 First 和 Follow 集。如果某个 `M[A, a]` 中没有填入产生式，就表示这是一个语法错误。如果某个格子被填入了多个产生式，说明这个文法不是 LL(1) 的。
            

#### **第 36-44 页：预测分析的实现**

- **内容简介**：讲解了两种实现 LL(1) 预测分析器的方法：递归下降和表驱动。
    
- **阐述**：
    
    - **第 37-40 页 (递归下降分析)**：
        
        - 为每个非终结符编写一个函数。
            
        - 函数的功能是识别该非终结符对应的语法结构。
            
        - 在函数内部，根据当前的 lookahead 符号，决定调用哪个其他非终结符的函数，或者匹配哪个终结符。
            
        - 这是一种非常直观和手动的实现方式，代码结构和文法结构高度对应。
            
    - **第 41-44 页 (表驱动的非递归分析)**：
        
        - 这是一种更通用、更自动化的方法。它需要三个组件：一个**栈**、一个**输入缓冲区**和一个**预测分析表 M**。
            
        - **工作流程**：
            
            1. 栈顶是 `X`，当前输入是 `a`。
                
            2. 如果 `X` 是终结符，就和 `a` 比较。匹配则都出栈/前移，不匹配则报错。
                
            3. 如果 `X` 是非终结符，查表 `M[X, a]`。
                
            4. 如果 `M[X, a]` 是产生式 `X → Y₁Y₂...Yₖ`，则将 `X` 出栈，然后把 `Yₖ, ..., Y₂, Y₁` 依次入栈（注意是反序）。
                
            5. 如果 `M[X, a]` 是空的，则报错。
                
            6. 重复此过程直到栈和输入都为空。
                

#### **第 45-48 页：非 LL(1) 文法**

- **内容简介**：总结了哪些文法不是 LL(1) 文法。
    
- **阐述**：
    
    - **二义性文法**肯定不是 LL(1) 文法，因为它天生就无法唯一确定分析树，这会直接导致预测分析表中出现冲突（一个格子有多个条目）。
        
    - **左递归文法**也不是 LL(1) 文法。消除左递归是使用自顶向下分析的前提。第 47-48 页用图示生动地解释了为什么左递归会导致无限循环。
        

---

### **第 49-55 页：语法错误的处理**

这部分内容讨论的是，当语法分析器发现输入不符合文法时，应该怎么办。

#### **第 49-50 页：错误处理目标与方法**

- **内容简介**：一个好的编译器不仅要能识别正确的程序，还要能在遇到错误时给出清晰的报告，并尝试从错误中恢复，继续检查后面的代码，以便一次性找出多个错误。
    
- **阐述**：
    
    - 主要介绍两种错误恢复策略：**恐慌模式 (Panic Mode)** 和 **短语层次恢复 (Phrase-level Recovery)**。
        

#### **第 51-54 页：恐慌模式**

- **内容简介**：这是最常用、最简单的错误恢复策略。
    
- **阐述**：
    
    - **思想**：当分析器遇到一个错误时，它会进入“恐慌”状态。它会不断地丢弃输入的符号，直到找到一个预定义的“**同步词法单元 (synchronizing token)**”为止。
        
    - **同步词法单元**通常是语言中明确的分隔符，比如分号 `;`、右括号 `}` 等。分析器认为，一旦看到这些符号，就意味着一个语法单元（如一条语句、一个函数体）可能结束了，可以从这里重新开始分析。
        
    - **如何确定同步集合？** 第 52 页给出了一些启发式规则，基本上就是利用 `Follow` 集（因为 `Follow(A)` 里的符号是合法地跟在 `A` 之后的）和 `First` 集来帮助确定。
        
    - **第 53-54 页**的例子展示了在分析表中加入 `synch` 条目，指导分析器在出错时如何跳过输入和弹出栈符号，以达到同步。
        

#### **第 55 页：短语层次的恢复**

- **内容简介**：介绍了一种更复杂但可能效果更好的恢复策略。
    
- **阐述**：
    
    - 它不只是简单地跳过输入，而是会尝试进行小范围的“修复”，比如在预测分析表的空白条目中填入特定的错误处理程序，这些程序可以插入一个缺失的分号、删除一个多余的逗号等。这种方法实现起来非常复杂，且要小心避免无限循环。
        

---

### **第 56 页：作业**

这页列出了本次课程的课后作业，主要是针对文法变换和计算 First/Follow 集的练习，这些都是本节课的核心技能。

---

### **总结**

这节课的内容环环相扣，逻辑性非常强：

1. **目标**：实现一个高效的**自顶向下语法分析器**。
    
2. **问题**：朴素的自顶向下分析会因为“选择困难”（多个产生式可选）而需要**回溯**，效率低下；同时还会因为**左递归**而陷入死循环。
    
3. **解决方案**：
    
    - 对文法进行**改造**：**消除左递归**、**提取左公因子**、**消除二义性**。
        
    - 目标是将文法转变为 **LL(1) 文法**。
        
4. **理论工具**：引入 **First 集**和 **Follow 集**来精确判断一个文法是否是 LL(1) 的，并用它们来构建**预测分析表**。
    
5. **实现**：基于预测分析表，可以用**递归下降**或**表驱动**的方法实现一个无需回溯的、高效的预测分析器。
    
6. **完善**：最后，讨论了实际编译器中必须考虑的**错误恢复**策略，如**恐慌模式**。
    

希望这份详细的讲解能帮助你完全理解这节课的内容！如果对某个具体知识点还有疑问，随时可以提出来。